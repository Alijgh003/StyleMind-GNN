{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJjCpDWBlSulYcRdN5TGXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alijgh003/StyleMind-GNN/blob/main/StyleMind_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ilOURpAYkjuf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ejZ_JBMqRQu",
        "outputId": "ff6b3375-597d-49c7-96de-c764b65c5403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "torch_version = str(torch.__version__)\n",
        "# scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "# sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "# !pip install torch-scatter -f $scatter_src\n",
        "# !pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "j54KJfvCqx5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c91729-b49d-4c46-ecad-a322b166d02c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = \"/content/drive/MyDrive/StyleMind-GNN\""
      ],
      "metadata": {
        "id": "zv1V_GrYq5i3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path"
      ],
      "metadata": {
        "id": "rgDdePkytykH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "items = pd.read_csv(path.join(datasets_path, \"new_items.csv\"))\n",
        "items.set_index(\"ID\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "twTtZX4HAX9j",
        "outputId": "a4ebf065-58ef-4cf8-b653-26d00527b80c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Unnamed: 0   index                                       url_name  \\\n",
              "ID                                                                             \n",
              "211990161           0       0                   neck print chiffon plus size   \n",
              "183179503           1       1  christian pellizzari floral jacquard trousers   \n",
              "152771755           2       2            natures jewelry stainless steel not   \n",
              "190445143           3       3                balmain women high waist flared   \n",
              "211444470           4       4                  acler orson high waist belted   \n",
              "...               ...     ...                                            ...   \n",
              "108411005      251003  251003          arnica embellished hand painted skirt   \n",
              "212452593      251004  251004                 thom browne little boy knitted   \n",
              "208651882      251005  251005              colorful vortex print shawl scarf   \n",
              "209210426      251006  251006               floral embroidered mesh see thru   \n",
              "181567392      251007  251007        shein sheinside extreme destroyed denim   \n",
              "\n",
              "                                                 description  \\\n",
              "ID                                                             \n",
              "211990161                                                NaN   \n",
              "183179503  Gold and black silk blend floral jacquard trou...   \n",
              "152771755                                                NaN   \n",
              "190445143  Decorative gold colored buttons with lion deta...   \n",
              "211444470  This Acler Orson High Waist Belted Short featu...   \n",
              "...                                                      ...   \n",
              "108411005  Shop Stella Jean in our expertly curated in-se...   \n",
              "212452593                                                NaN   \n",
              "208651882                                                NaN   \n",
              "209210426                                                NaN   \n",
              "181567392                                                NaN   \n",
              "\n",
              "                                                  catgeories  \\\n",
              "ID                                                             \n",
              "211990161                                                NaN   \n",
              "183179503  [\"Women's Fashion\", 'Clothing', 'Pants', 'Chri...   \n",
              "152771755                                                NaN   \n",
              "190445143  [\"Women's Fashion\", 'Clothing', 'Pants', 'Balm...   \n",
              "211444470  [\"Women's Fashion\", 'Clothing', 'Shorts', 'Mar...   \n",
              "...                                                      ...   \n",
              "108411005  [\"Women's Fashion\", 'Clothing', 'Skirts', 'Kne...   \n",
              "212452593                                                NaN   \n",
              "208651882                                                NaN   \n",
              "209210426                                                NaN   \n",
              "181567392                                                NaN   \n",
              "\n",
              "                                                   title  \\\n",
              "ID                                                         \n",
              "211990161                                            NaN   \n",
              "183179503  Christian Pellizzari floral jacquard trousers   \n",
              "152771755                                            NaN   \n",
              "190445143     Balmain Women High Waist Flared Knit Pants   \n",
              "211444470            Acler Orson High Waist Belted Short   \n",
              "...                                                  ...   \n",
              "108411005          Arnica Embellished Hand Painted Skirt   \n",
              "212452593                                            NaN   \n",
              "208651882                                            NaN   \n",
              "209210426                                            NaN   \n",
              "181567392                                            NaN   \n",
              "\n",
              "                                                     related  category_id  \\\n",
              "ID                                                                          \n",
              "211990161                                                NaN            3   \n",
              "183179503  ['Floral pants', 'Grey pants', 'Print pants', ...            2   \n",
              "152771755                                                NaN           11   \n",
              "190445143  ['Balmain', 'Flared pants', 'High-waisted pant...            2   \n",
              "211444470  ['Short shorts', 'High-waisted shorts', 'High ...            2   \n",
              "...                                                      ...          ...   \n",
              "108411005                 ['Red skirt', 'Embellished skirt']            2   \n",
              "212452593                                                NaN            4   \n",
              "208651882                                                NaN            8   \n",
              "209210426                                                NaN            3   \n",
              "181567392                                                NaN            4   \n",
              "\n",
              "          semantic_category  name_len  \n",
              "ID                                     \n",
              "211990161              tops        28  \n",
              "183179503           bottoms        45  \n",
              "152771755         jewellery        35  \n",
              "190445143           bottoms        31  \n",
              "211444470           bottoms        29  \n",
              "...                     ...       ...  \n",
              "108411005           bottoms        37  \n",
              "212452593         outerwear        30  \n",
              "208651882           scarves        33  \n",
              "209210426              tops        32  \n",
              "181567392         outerwear        39  \n",
              "\n",
              "[251008 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bac84286-449c-4a5e-ad5b-a91d3cd1396d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>url_name</th>\n",
              "      <th>description</th>\n",
              "      <th>catgeories</th>\n",
              "      <th>title</th>\n",
              "      <th>related</th>\n",
              "      <th>category_id</th>\n",
              "      <th>semantic_category</th>\n",
              "      <th>name_len</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>211990161</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>neck print chiffon plus size</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>tops</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183179503</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>christian pellizzari floral jacquard trousers</td>\n",
              "      <td>Gold and black silk blend floral jacquard trou...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Pants', 'Chri...</td>\n",
              "      <td>Christian Pellizzari floral jacquard trousers</td>\n",
              "      <td>['Floral pants', 'Grey pants', 'Print pants', ...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152771755</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>natures jewelry stainless steel not</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>jewellery</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190445143</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>balmain women high waist flared</td>\n",
              "      <td>Decorative gold colored buttons with lion deta...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Pants', 'Balm...</td>\n",
              "      <td>Balmain Women High Waist Flared Knit Pants</td>\n",
              "      <td>['Balmain', 'Flared pants', 'High-waisted pant...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211444470</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>acler orson high waist belted</td>\n",
              "      <td>This Acler Orson High Waist Belted Short featu...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Shorts', 'Mar...</td>\n",
              "      <td>Acler Orson High Waist Belted Short</td>\n",
              "      <td>['Short shorts', 'High-waisted shorts', 'High ...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108411005</th>\n",
              "      <td>251003</td>\n",
              "      <td>251003</td>\n",
              "      <td>arnica embellished hand painted skirt</td>\n",
              "      <td>Shop Stella Jean in our expertly curated in-se...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Skirts', 'Kne...</td>\n",
              "      <td>Arnica Embellished Hand Painted Skirt</td>\n",
              "      <td>['Red skirt', 'Embellished skirt']</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212452593</th>\n",
              "      <td>251004</td>\n",
              "      <td>251004</td>\n",
              "      <td>thom browne little boy knitted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>outerwear</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208651882</th>\n",
              "      <td>251005</td>\n",
              "      <td>251005</td>\n",
              "      <td>colorful vortex print shawl scarf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>scarves</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209210426</th>\n",
              "      <td>251006</td>\n",
              "      <td>251006</td>\n",
              "      <td>floral embroidered mesh see thru</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>tops</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181567392</th>\n",
              "      <td>251007</td>\n",
              "      <td>251007</td>\n",
              "      <td>shein sheinside extreme destroyed denim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>outerwear</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>251008 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bac84286-449c-4a5e-ad5b-a91d3cd1396d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bac84286-449c-4a5e-ad5b-a91d3cd1396d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bac84286-449c-4a5e-ad5b-a91d3cd1396d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c1e11a18-9e8c-4b91-9238-e54c64812e4e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1e11a18-9e8c-4b91-9238-e54c64812e4e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c1e11a18-9e8c-4b91-9238-e54c64812e4e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.load(path.join(datasets_path, \"items_img2vec.pth\"))\n",
        "x, x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S81FJj9utstn",
        "outputId": "de66cf5e-a205-44d1-b773-ef88792cd649"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.2134, 0.4643, 0.3334,  ..., 0.3668, 0.0379, 0.0898],\n",
              "         [0.5982, 0.7184, 0.2694,  ..., 0.2122, 0.6592, 0.0720],\n",
              "         [0.3819, 4.5064, 0.2355,  ..., 0.2252, 0.7095, 0.4182],\n",
              "         ...,\n",
              "         [0.0192, 0.2306, 0.6947,  ..., 0.2099, 0.2533, 0.1645],\n",
              "         [0.4130, 0.2520, 0.2924,  ..., 0.5590, 0.3986, 0.0838],\n",
              "         [0.0576, 1.2712, 1.8085,  ..., 0.6934, 0.9322, 0.0889]]),\n",
              " torch.Size([251008, 2048]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.dtype ,x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQG4pouWuLVH",
        "outputId": "07dd8829-d09e-4d2a-8d92-4363686c6cf7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.Size([251008, 2048]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_indices_path = path.join(datasets_path, \"dataset\", \"polyvore_outfits\", \"nondisjoint\")"
      ],
      "metadata": {
        "id": "9mvzirN8tyMK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index = torch.load(path.join(edge_indices_path, \"train_edge_index.pth\"))\n",
        "test_edge_index = torch.load(path.join(edge_indices_path, \"test_edge_index.pth\"))\n",
        "valid_edge_index = torch.load(path.join(edge_indices_path, \"valid_edge_index.pth\"))"
      ],
      "metadata": {
        "id": "QpssZBHZuUue"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index.shape, valid_edge_index.shape, test_edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFN1Dp9um3w",
        "outputId": "9c16c53f-bda2-4e44-ed08-62d18abb5b9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([53306, 2, 171]),\n",
              " torch.Size([5000, 2, 91]),\n",
              " torch.Size([10000, 2, 136]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_train_edge_index = torch.load(path.join(edge_indices_path,\"negative_train_edge_index.pth\"))\n",
        "\n",
        "negative_train_edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8EFs4H-HhS9",
        "outputId": "cd09571a-d5b7-4b5e-a453-6d7f0b612a1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([53306, 2, 171])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_node_embedding_with_mask_meaningless_edge_index_elements(x, edge_index):\n",
        "  mask = torch.where(edge_index==-1, torch.tensor(0), torch.tensor(1)).unsqueeze(-1)\n",
        "  r = x[edge_index] * mask\n",
        "  return r"
      ],
      "metadata": {
        "id": "n8pAOUMsqTxH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_edge_indices(edge_index_tensor_by_groups):\n",
        "  result = torch.cat([edge_index_tensor_by_groups[i] for i in range(edge_index_tensor_by_groups.shape[0])],dim=-1)\n",
        "  result = result[:,torch.where(result[0]>-1)[0]]\n",
        "  return result"
      ],
      "metadata": {
        "id": "vMyux7JYvApm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_edge_indices(train_edge_index[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCreRAxilaHj",
        "outputId": "56a667eb-0cf7-419a-9498-93a9f7c1c092"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[204439, 154376, 154376, 154376, 154376, 154376, 154376, 154376, 191980,\n",
              "         191980, 191980, 192270, 192270, 192270, 192270,   6713,   6713,  78057,\n",
              "          78057,  78057,  78057,  78057, 100757, 100757, 100757, 100757, 100757,\n",
              "         100757],\n",
              "        [249298, 249298, 204439, 191980, 192270,   6713,  78057, 100757, 249298,\n",
              "         204439,   6713, 249298, 204439, 191980,   6713, 249298, 204439, 249298,\n",
              "         204439, 191980, 192270,   6713, 249298, 204439, 191980, 192270,   6713,\n",
              "          78057]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKBOTWU-09nR",
        "outputId": "c8d162ea-f09a-4e35-d862-3711076d24d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([251008, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv,MessagePassing\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "n78uETa0vUTG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGCNMLP(MessagePassing):\n",
        "  def __init__(self,in_channels, out_channels):\n",
        "    super(MyGCNMLP, self).__init__()\n",
        "\n",
        "    self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    return self.lin(x)"
      ],
      "metadata": {
        "id": "RANzvxNZJatA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleMindGCN(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels ,number_of_layers, model_type=\"GCN\", dropout=0.5):\n",
        "    super(StyleMindGCN,self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList([self.get_model(model_type)(in_channels if i==0 else hidden_channels,hidden_channels if i<number_of_layers-1 else out_channels) for i in range(number_of_layers)])\n",
        "    self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_channels) for i in range(number_of_layers-1)])\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "        conv.reset_parameters()\n",
        "    for bn in self.bns:\n",
        "        bn.reset_parameters()\n",
        "\n",
        "\n",
        "  def get_model(self, model_type):\n",
        "    if(model_type==\"GAT\"):\n",
        "      return GATConv\n",
        "    elif(model_type==\"MLP\"):\n",
        "      return MyGCNMLP\n",
        "\n",
        "    else:\n",
        "      return GCNConv\n",
        "\n",
        "  def forward(self, x, edge_index, outfits):\n",
        "    # x is the tensor of node_features (i.e. ResNet representation of each image of each garment)\n",
        "    # edge_index is the (2, |E|) to represent edges between garments\n",
        "    # outfits are 3 dim tensors with shape of (|O|, 2 , |Maximum combination of garments in the outfits in dataset|)\n",
        "    edge_index = to_undirected(edge_index)\n",
        "\n",
        "    for conv, bn in zip(self.convs[:-1], self.bns):\n",
        "      x = conv(x, edge_index)\n",
        "      x = bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, self.dropout)\n",
        "\n",
        "    x = self.convs[-1](x,edge_index)\n",
        "\n",
        "    o = get_node_embedding_with_mask_meaningless_edge_index_elements(x,outfits)\n",
        "    # the output will be in the shape of (|O|, 2, |Maximum combination of garments in the outfits in dataset|)\n",
        "    o = (o[:,0,:] * o[:,1,:]).sum(dim=(-1,-2)) / (outfits[:,:1,:]>=0).float().sum(dim=(-1,-2))\n",
        "    # the output will be in the shape of |O|\n",
        "    o = F.sigmoid(o)\n",
        "    return x, o\n",
        "\n"
      ],
      "metadata": {
        "id": "hiDfVi00vJdO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_UIhe-3K5yUS",
        "outputId": "0ba2d023-110d-4241-b5d0-0b0bacce5a2d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, x, edge_index, outfits, labels, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  embds,o = model(x, edge_index, outfits)\n",
        "\n",
        "  loss = loss_fn(o, labels)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "km5pFkE_5CLC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, x, edge_index, outfits, labels):\n",
        "  model.eval()\n",
        "\n",
        "  embds,o = model(x, edge_index, outfits)\n",
        "  o_pred = torch.where(o>=0.5, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "  acc = (o_pred==labels).float().sum() / o_pred.shape[0]\n",
        "\n",
        "  return  acc"
      ],
      "metadata": {
        "id": "83SDJGCQ7Khn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = x.shape[-1]\n",
        "hidden_channels = 128\n",
        "out_channels = 128\n",
        "lr = 0.01\n",
        "number_of_layers = 3\n",
        "dropout = 0.5"
      ],
      "metadata": {
        "id": "_9GDYCrF8hNW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StyleMindGCN(in_channels, hidden_channels\n",
        "                     ,out_channels, number_of_layers,\"GCN\", dropout).to(device)"
      ],
      "metadata": {
        "id": "vI9dHMxn8lRi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ZNMMLHNbff",
        "outputId": "f13ef167-d5c6-4bc4-e750-8b258f5e95a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ngQnrxKxNi84"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_RQ-ogiLUWp",
        "outputId": "d5cf9ff4-45df-4d3e-dd9f-63ba5ebe71fb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory allocated: 3474.02 MB\n",
            "Memory reserved: 7214.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoKmKHQuJdmo",
        "outputId": "15a9b241-eb30-4624-ff38-41b73482cbf8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([53306, 2, 171])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 300"
      ],
      "metadata": {
        "id": "eoKQAKK-9tLB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "- [x] splitting train dataset to two part\n",
        "      - message_passing\n",
        "      - labels\n",
        "- [x] negative sampling  \n",
        "  naive negative sampling: We just have randomly selected\n",
        "    8 nodes to combine(2,nodes) to create some negative samples"
      ],
      "metadata": {
        "id": "HRYH1FTNEMz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def split_train_data(train_edge_index, diameter=8):\n",
        "  edge_nums =  train_edge_index.shape[0]\n",
        "  random_permutation = torch.randperm(edge_nums)\n",
        "  eval_outfits = train_edge_index[random_permutation[:edge_nums//diameter]]\n",
        "  message_passing_edges = concat_edge_indices(train_edge_index[random_permutation[edge_nums//diameter:]])\n",
        "  return eval_outfits, message_passing_edges\n",
        "\n",
        "a = split_train_data(train_edge_index)\n",
        "[value.shape for  value in a]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPILhMhvEyqD",
        "outputId": "ac80997c-ab31-4c77-993f-cf41b07e9c2e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 264 ms, sys: 204 ms, total: 468 ms\n",
            "Wall time: 472 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([6663, 2, 171]), torch.Size([2, 601718])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive negative sampling"
      ],
      "metadata": {
        "id": "gMQC8BPG6_J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "def create_random_negative_samples(num_set, num_samples, num_garment, padding_size):\n",
        "  combinations = [[list(itertools.combinations(torch.randperm(x.shape[0])[:num_garment].tolist(), 2)) for i in range(num_samples)] for j in range(num_set)]\n",
        "\n",
        "  tensor = torch.tensor(combinations).transpose(-1,-2)\n",
        "  tensor = torch.cat((tensor.flip(-2), tensor), dim=-1)\n",
        "  result = torch.full((num_set, num_samples, 2, padding_size), -1)\n",
        "  result[:,:,:,:tensor.shape[-1]] = tensor\n",
        "  return result\n",
        "\n",
        "# negative_samples = create_random_negative_samples(10, 5000, 8, train_edge_index.shape[-1])\n",
        "# negative_samples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q0oLDysG7ym",
        "outputId": "662e5ed8-3317-433f-9038-c0dc891b1d00"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11 µs, sys: 0 ns, total: 11 µs\n",
            "Wall time: 15 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_negative_samples = create_random_negative_samples(1, valid_edge_index.shape[0],8, valid_edge_index.shape[-1])[0]\n",
        "test_negative_samples = create_random_negative_samples(1, test_edge_index.shape[0],8, test_edge_index.shape[-1])[0]\n",
        "valid_negative_samples.shape, test_negative_samples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjoBWZt0v-15",
        "outputId": "33fc069c-c14b-4a38-dc8f-7496e23b0223"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5000, 2, 91]), torch.Size([10000, 2, 136]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_parameters()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "x = x.to(device)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    train_positives_eval_outfits, message_passing_edges = split_train_data(train_edge_index, 8)\n",
        "    train_negatives_eval_outfits = negative_samples[epoch % negative_samples.shape[0]]\n",
        "    train_labels = torch.cat((torch.ones(train_positives_eval_outfits.shape[0]),\n",
        "                              torch.zeros(train_negatives_eval_outfits.shape[0])))\n",
        "    train_eval_outfits = torch.cat((train_positives_eval_outfits, train_negatives_eval_outfits), dim=0)\n",
        "    train_loss = train(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device),\n",
        "                       loss_fn=loss_fn, optimizer=optimizer)\n",
        "    train_acc = test(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device))\n",
        "\n",
        "\n",
        "    valid_message_passing_edges = torch.cat((concat_edge_indices(train_positives_eval_outfits)\n",
        "    , message_passing_edges), dim=-1)\n",
        "    valid_labels = torch.cat((torch.ones(valid_edge_index.shape[0]),\n",
        "                              torch.zeros(valid_negative_samples.shape[0])))\n",
        "    valid_eval_outfits = torch.cat((valid_edge_index, valid_negative_samples), dim=0)\n",
        "    valid_acc = test(model, x, valid_message_passing_edges.to(device)\n",
        "    , valid_eval_outfits.to(device), valid_labels.to(device))\n",
        "\n",
        "\n",
        "\n",
        "    test_message_passing_edges = torch.cat((concat_edge_indices(valid_edge_index)\n",
        "    , valid_message_passing_edges), dim=-1)\n",
        "    test_labels = torch.cat((torch.ones(test_edge_index.shape[0]),\n",
        "                              torch.zeros(test_negative_samples.shape[0])))\n",
        "    test_eval_outfits = torch.cat((test_edge_index, test_negative_samples), dim=0)\n",
        "    test_acc = test(model, x, test_message_passing_edges.to(device)\n",
        "    , test_eval_outfits.to(device), test_labels.to(device))\n",
        "\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {train_loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "XfIBGXzF9y51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "109feb72-fad4-4087-8481-0fd6d5bb3b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 15.5558, Train: 57.13%, Valid: 50.00% Test: 49.99%\n",
            "Epoch: 01, Loss: 6.2417, Train: 57.02%, Valid: 49.93% Test: 49.92%\n",
            "Epoch: 02, Loss: 2.2776, Train: 57.07%, Valid: 49.98% Test: 49.95%\n",
            "Epoch: 03, Loss: 4.9023, Train: 57.10%, Valid: 49.97% Test: 49.97%\n",
            "Epoch: 04, Loss: 4.2414, Train: 57.06%, Valid: 49.93% Test: 49.95%\n",
            "Epoch: 05, Loss: 2.1029, Train: 57.04%, Valid: 49.97% Test: 49.94%\n",
            "Epoch: 06, Loss: 1.7390, Train: 57.10%, Valid: 49.99% Test: 49.98%\n",
            "Epoch: 07, Loss: 2.0399, Train: 57.10%, Valid: 49.98% Test: 50.00%\n",
            "Epoch: 08, Loss: 2.0631, Train: 57.12%, Valid: 49.98% Test: 49.99%\n",
            "Epoch: 09, Loss: 1.8429, Train: 57.04%, Valid: 49.93% Test: 49.92%\n",
            "Epoch: 10, Loss: 1.3993, Train: 57.04%, Valid: 49.92% Test: 49.88%\n",
            "Epoch: 11, Loss: 1.2240, Train: 56.98%, Valid: 49.81% Test: 49.76%\n",
            "Epoch: 12, Loss: 1.0704, Train: 56.88%, Valid: 49.83% Test: 49.83%\n",
            "Epoch: 13, Loss: 1.0720, Train: 56.97%, Valid: 49.80% Test: 49.79%\n",
            "Epoch: 14, Loss: 1.0925, Train: 56.81%, Valid: 49.62% Test: 49.79%\n",
            "Epoch: 15, Loss: 0.9955, Train: 56.76%, Valid: 49.56% Test: 49.67%\n",
            "Epoch: 16, Loss: 0.9425, Train: 56.34%, Valid: 49.36% Test: 49.39%\n",
            "Epoch: 17, Loss: 0.9156, Train: 56.13%, Valid: 49.22% Test: 49.22%\n",
            "Epoch: 18, Loss: 0.8003, Train: 56.04%, Valid: 49.11% Test: 49.31%\n",
            "Epoch: 19, Loss: 0.7914, Train: 55.98%, Valid: 48.94% Test: 49.11%\n",
            "Epoch: 20, Loss: 0.7957, Train: 56.01%, Valid: 49.07% Test: 49.20%\n",
            "Epoch: 21, Loss: 0.7691, Train: 56.05%, Valid: 49.16% Test: 49.22%\n",
            "Epoch: 22, Loss: 0.7645, Train: 56.19%, Valid: 49.06% Test: 49.21%\n",
            "Epoch: 23, Loss: 0.7331, Train: 56.03%, Valid: 49.17% Test: 49.22%\n",
            "Epoch: 24, Loss: 0.6897, Train: 55.98%, Valid: 49.15% Test: 49.04%\n",
            "Epoch: 25, Loss: 0.6851, Train: 55.85%, Valid: 48.83% Test: 48.96%\n",
            "Epoch: 26, Loss: 0.6683, Train: 56.01%, Valid: 48.93% Test: 48.77%\n",
            "Epoch: 27, Loss: 0.6369, Train: 55.83%, Valid: 48.77% Test: 48.84%\n",
            "Epoch: 28, Loss: 0.6428, Train: 55.82%, Valid: 48.92% Test: 49.09%\n",
            "Epoch: 29, Loss: 0.6425, Train: 56.09%, Valid: 49.16% Test: 49.07%\n",
            "Epoch: 30, Loss: 0.6131, Train: 55.92%, Valid: 49.38% Test: 49.21%\n",
            "Epoch: 31, Loss: 0.6376, Train: 56.12%, Valid: 49.25% Test: 49.37%\n",
            "Epoch: 32, Loss: 0.5964, Train: 56.52%, Valid: 49.35% Test: 49.33%\n",
            "Epoch: 33, Loss: 0.5708, Train: 56.79%, Valid: 49.58% Test: 49.53%\n",
            "Epoch: 34, Loss: 0.5876, Train: 56.89%, Valid: 50.00% Test: 49.61%\n",
            "Epoch: 35, Loss: 0.5767, Train: 57.24%, Valid: 50.27% Test: 50.19%\n",
            "Epoch: 36, Loss: 0.5835, Train: 57.83%, Valid: 50.97% Test: 50.53%\n",
            "Epoch: 37, Loss: 0.5609, Train: 58.58%, Valid: 52.30% Test: 51.28%\n",
            "Epoch: 38, Loss: 0.5654, Train: 58.80%, Valid: 52.80% Test: 51.73%\n",
            "Epoch: 39, Loss: 0.5661, Train: 59.26%, Valid: 53.08% Test: 52.31%\n",
            "Epoch: 40, Loss: 0.5397, Train: 59.48%, Valid: 53.17% Test: 52.67%\n",
            "Epoch: 41, Loss: 0.5574, Train: 59.57%, Valid: 54.09% Test: 52.96%\n",
            "Epoch: 42, Loss: 0.5372, Train: 60.10%, Valid: 54.35% Test: 53.61%\n",
            "Epoch: 43, Loss: 0.5248, Train: 60.95%, Valid: 55.00% Test: 54.24%\n",
            "Epoch: 44, Loss: 0.5361, Train: 61.22%, Valid: 55.86% Test: 55.02%\n",
            "Epoch: 45, Loss: 0.5302, Train: 62.45%, Valid: 57.21% Test: 55.84%\n",
            "Epoch: 46, Loss: 0.5262, Train: 62.72%, Valid: 58.37% Test: 57.02%\n",
            "Epoch: 47, Loss: 0.5336, Train: 64.31%, Valid: 59.23% Test: 58.15%\n",
            "Epoch: 48, Loss: 0.5169, Train: 64.31%, Valid: 60.31% Test: 59.45%\n",
            "Epoch: 49, Loss: 0.5239, Train: 65.69%, Valid: 61.41% Test: 60.13%\n",
            "Epoch: 50, Loss: 0.5113, Train: 66.33%, Valid: 62.32% Test: 60.97%\n",
            "Epoch: 51, Loss: 0.5176, Train: 66.65%, Valid: 63.13% Test: 62.10%\n",
            "Epoch: 52, Loss: 0.5101, Train: 67.65%, Valid: 63.78% Test: 62.77%\n",
            "Epoch: 53, Loss: 0.4994, Train: 67.91%, Valid: 64.60% Test: 63.44%\n",
            "Epoch: 54, Loss: 0.5090, Train: 67.60%, Valid: 65.68% Test: 64.52%\n",
            "Epoch: 55, Loss: 0.4994, Train: 69.25%, Valid: 66.60% Test: 65.88%\n",
            "Epoch: 56, Loss: 0.5061, Train: 70.27%, Valid: 67.27% Test: 66.49%\n",
            "Epoch: 57, Loss: 0.5047, Train: 70.90%, Valid: 68.49% Test: 67.72%\n",
            "Epoch: 58, Loss: 0.4953, Train: 71.41%, Valid: 69.78% Test: 68.14%\n",
            "Epoch: 59, Loss: 0.4973, Train: 72.00%, Valid: 70.50% Test: 69.54%\n",
            "Epoch: 60, Loss: 0.4954, Train: 72.93%, Valid: 71.09% Test: 69.58%\n",
            "Epoch: 61, Loss: 0.4922, Train: 72.81%, Valid: 71.09% Test: 70.10%\n",
            "Epoch: 62, Loss: 0.5084, Train: 73.43%, Valid: 71.55% Test: 70.09%\n",
            "Epoch: 63, Loss: 0.4861, Train: 73.31%, Valid: 71.59% Test: 70.18%\n",
            "Epoch: 64, Loss: 0.5042, Train: 72.99%, Valid: 71.73% Test: 70.52%\n",
            "Epoch: 65, Loss: 0.4881, Train: 73.45%, Valid: 71.82% Test: 70.84%\n",
            "Epoch: 66, Loss: 0.4922, Train: 73.58%, Valid: 72.53% Test: 71.41%\n",
            "Epoch: 67, Loss: 0.4884, Train: 73.98%, Valid: 73.26% Test: 71.20%\n",
            "Epoch: 68, Loss: 0.4864, Train: 74.34%, Valid: 73.20% Test: 71.56%\n",
            "Epoch: 69, Loss: 0.4872, Train: 73.43%, Valid: 73.43% Test: 71.90%\n",
            "Epoch: 70, Loss: 0.4830, Train: 73.75%, Valid: 73.28% Test: 72.26%\n",
            "Epoch: 71, Loss: 0.4913, Train: 73.77%, Valid: 73.39% Test: 71.87%\n",
            "Epoch: 72, Loss: 0.4756, Train: 74.91%, Valid: 73.22% Test: 71.93%\n",
            "Epoch: 73, Loss: 0.4844, Train: 74.28%, Valid: 73.19% Test: 71.40%\n",
            "Epoch: 74, Loss: 0.4826, Train: 74.29%, Valid: 73.15% Test: 70.99%\n",
            "Epoch: 75, Loss: 0.4729, Train: 75.16%, Valid: 72.66% Test: 71.72%\n",
            "Epoch: 76, Loss: 0.4781, Train: 74.31%, Valid: 72.90% Test: 71.41%\n",
            "Epoch: 77, Loss: 0.4828, Train: 74.06%, Valid: 72.94% Test: 71.23%\n",
            "Epoch: 78, Loss: 0.4766, Train: 74.95%, Valid: 73.59% Test: 71.45%\n",
            "Epoch: 79, Loss: 0.4785, Train: 74.67%, Valid: 73.47% Test: 71.37%\n",
            "Epoch: 80, Loss: 0.4702, Train: 75.55%, Valid: 74.00% Test: 72.19%\n",
            "Epoch: 81, Loss: 0.4774, Train: 74.75%, Valid: 74.10% Test: 72.05%\n",
            "Epoch: 82, Loss: 0.4784, Train: 75.42%, Valid: 74.42% Test: 72.40%\n",
            "Epoch: 83, Loss: 0.4714, Train: 75.15%, Valid: 74.03% Test: 72.53%\n",
            "Epoch: 84, Loss: 0.4787, Train: 74.54%, Valid: 74.60% Test: 72.38%\n",
            "Epoch: 85, Loss: 0.4680, Train: 75.11%, Valid: 74.47% Test: 72.21%\n",
            "Epoch: 86, Loss: 0.4715, Train: 75.40%, Valid: 74.42% Test: 72.53%\n",
            "Epoch: 87, Loss: 0.4742, Train: 75.44%, Valid: 74.77% Test: 72.53%\n",
            "Epoch: 88, Loss: 0.4664, Train: 75.02%, Valid: 74.30% Test: 72.25%\n",
            "Epoch: 89, Loss: 0.4688, Train: 75.17%, Valid: 74.59% Test: 72.97%\n",
            "Epoch: 90, Loss: 0.4677, Train: 76.10%, Valid: 73.99% Test: 72.87%\n",
            "Epoch: 91, Loss: 0.4724, Train: 75.98%, Valid: 74.81% Test: 73.31%\n",
            "Epoch: 92, Loss: 0.4613, Train: 76.58%, Valid: 74.80% Test: 72.93%\n",
            "Epoch: 93, Loss: 0.4633, Train: 76.28%, Valid: 74.65% Test: 73.40%\n",
            "Epoch: 94, Loss: 0.4663, Train: 75.99%, Valid: 75.42% Test: 73.36%\n",
            "Epoch: 95, Loss: 0.4616, Train: 76.60%, Valid: 75.02% Test: 73.42%\n",
            "Epoch: 96, Loss: 0.4660, Train: 76.31%, Valid: 74.47% Test: 72.96%\n",
            "Epoch: 97, Loss: 0.4659, Train: 76.04%, Valid: 74.54% Test: 72.88%\n",
            "Epoch: 98, Loss: 0.4661, Train: 76.01%, Valid: 74.28% Test: 72.52%\n",
            "Epoch: 99, Loss: 0.4622, Train: 76.52%, Valid: 74.73% Test: 72.27%\n",
            "Epoch: 100, Loss: 0.4621, Train: 76.34%, Valid: 74.38% Test: 72.36%\n",
            "Epoch: 101, Loss: 0.4625, Train: 76.04%, Valid: 74.52% Test: 72.48%\n",
            "Epoch: 102, Loss: 0.4544, Train: 76.55%, Valid: 74.42% Test: 72.49%\n",
            "Epoch: 103, Loss: 0.4628, Train: 75.88%, Valid: 74.50% Test: 72.58%\n",
            "Epoch: 104, Loss: 0.4589, Train: 75.98%, Valid: 74.98% Test: 72.56%\n",
            "Epoch: 105, Loss: 0.4585, Train: 77.83%, Valid: 75.01% Test: 72.74%\n",
            "Epoch: 106, Loss: 0.4551, Train: 76.67%, Valid: 74.32% Test: 72.17%\n",
            "Epoch: 107, Loss: 0.4589, Train: 76.62%, Valid: 74.49% Test: 72.11%\n",
            "Epoch: 108, Loss: 0.4561, Train: 76.11%, Valid: 74.48% Test: 72.15%\n",
            "Epoch: 109, Loss: 0.4534, Train: 76.59%, Valid: 74.29% Test: 72.60%\n",
            "Epoch: 110, Loss: 0.4567, Train: 76.52%, Valid: 74.76% Test: 72.86%\n",
            "Epoch: 111, Loss: 0.4651, Train: 76.03%, Valid: 74.57% Test: 73.33%\n",
            "Epoch: 112, Loss: 0.4523, Train: 76.68%, Valid: 74.54% Test: 73.18%\n",
            "Epoch: 113, Loss: 0.4518, Train: 76.70%, Valid: 75.15% Test: 73.11%\n",
            "Epoch: 114, Loss: 0.4623, Train: 76.90%, Valid: 75.08% Test: 73.10%\n",
            "Epoch: 115, Loss: 0.4547, Train: 76.71%, Valid: 74.89% Test: 72.83%\n",
            "Epoch: 116, Loss: 0.4567, Train: 76.58%, Valid: 74.65% Test: 72.63%\n",
            "Epoch: 117, Loss: 0.4548, Train: 76.64%, Valid: 74.05% Test: 72.42%\n",
            "Epoch: 118, Loss: 0.4616, Train: 76.30%, Valid: 74.44% Test: 72.26%\n",
            "Epoch: 119, Loss: 0.4526, Train: 76.52%, Valid: 74.70% Test: 72.50%\n",
            "Epoch: 120, Loss: 0.4562, Train: 76.28%, Valid: 74.16% Test: 72.57%\n",
            "Epoch: 121, Loss: 0.4488, Train: 76.70%, Valid: 75.06% Test: 72.73%\n",
            "Epoch: 122, Loss: 0.4510, Train: 76.61%, Valid: 74.95% Test: 73.11%\n",
            "Epoch: 123, Loss: 0.4519, Train: 76.47%, Valid: 75.01% Test: 73.25%\n",
            "Epoch: 124, Loss: 0.4484, Train: 76.76%, Valid: 74.86% Test: 73.68%\n",
            "Epoch: 125, Loss: 0.4452, Train: 76.69%, Valid: 74.85% Test: 72.96%\n",
            "Epoch: 126, Loss: 0.4576, Train: 76.06%, Valid: 74.87% Test: 73.10%\n",
            "Epoch: 127, Loss: 0.4561, Train: 76.34%, Valid: 74.86% Test: 73.24%\n",
            "Epoch: 128, Loss: 0.4461, Train: 76.39%, Valid: 74.89% Test: 73.11%\n",
            "Epoch: 129, Loss: 0.4493, Train: 75.61%, Valid: 74.47% Test: 73.21%\n",
            "Epoch: 130, Loss: 0.4540, Train: 75.81%, Valid: 74.85% Test: 73.62%\n",
            "Epoch: 131, Loss: 0.4462, Train: 76.58%, Valid: 74.45% Test: 73.60%\n",
            "Epoch: 132, Loss: 0.4439, Train: 77.17%, Valid: 75.27% Test: 73.33%\n",
            "Epoch: 133, Loss: 0.4506, Train: 76.64%, Valid: 75.25% Test: 73.11%\n",
            "Epoch: 134, Loss: 0.4463, Train: 76.70%, Valid: 74.98% Test: 73.61%\n",
            "Epoch: 135, Loss: 0.4454, Train: 76.53%, Valid: 75.34% Test: 73.47%\n",
            "Epoch: 136, Loss: 0.4522, Train: 76.78%, Valid: 75.20% Test: 73.82%\n",
            "Epoch: 137, Loss: 0.4481, Train: 76.82%, Valid: 75.82% Test: 73.52%\n",
            "Epoch: 138, Loss: 0.4506, Train: 76.94%, Valid: 75.43% Test: 73.75%\n",
            "Epoch: 139, Loss: 0.4467, Train: 77.17%, Valid: 75.76% Test: 73.72%\n",
            "Epoch: 140, Loss: 0.4440, Train: 77.32%, Valid: 75.49% Test: 73.98%\n",
            "Epoch: 141, Loss: 0.4410, Train: 77.36%, Valid: 75.86% Test: 74.82%\n",
            "Epoch: 142, Loss: 0.4413, Train: 77.66%, Valid: 75.85% Test: 74.51%\n",
            "Epoch: 143, Loss: 0.4421, Train: 77.65%, Valid: 76.48% Test: 75.04%\n",
            "Epoch: 144, Loss: 0.4443, Train: 76.53%, Valid: 75.96% Test: 74.61%\n",
            "Epoch: 145, Loss: 0.4388, Train: 78.01%, Valid: 76.19% Test: 75.12%\n",
            "Epoch: 146, Loss: 0.4495, Train: 78.01%, Valid: 76.19% Test: 74.57%\n",
            "Epoch: 147, Loss: 0.4486, Train: 77.73%, Valid: 75.88% Test: 73.83%\n",
            "Epoch: 148, Loss: 0.4439, Train: 77.93%, Valid: 75.30% Test: 74.00%\n",
            "Epoch: 149, Loss: 0.4371, Train: 77.82%, Valid: 76.04% Test: 74.08%\n",
            "Epoch: 150, Loss: 0.4427, Train: 77.52%, Valid: 76.39% Test: 74.74%\n",
            "Epoch: 151, Loss: 0.4368, Train: 77.80%, Valid: 76.06% Test: 74.61%\n",
            "Epoch: 152, Loss: 0.4390, Train: 77.60%, Valid: 76.16% Test: 74.71%\n",
            "Epoch: 153, Loss: 0.4397, Train: 77.52%, Valid: 75.93% Test: 74.39%\n",
            "Epoch: 154, Loss: 0.4357, Train: 77.81%, Valid: 76.31% Test: 74.78%\n",
            "Epoch: 155, Loss: 0.4398, Train: 78.02%, Valid: 76.32% Test: 74.44%\n",
            "Epoch: 156, Loss: 0.4445, Train: 77.54%, Valid: 76.37% Test: 74.25%\n",
            "Epoch: 157, Loss: 0.4420, Train: 77.44%, Valid: 76.29% Test: 74.13%\n",
            "Epoch: 158, Loss: 0.4422, Train: 77.81%, Valid: 76.39% Test: 74.28%\n",
            "Epoch: 159, Loss: 0.4402, Train: 77.60%, Valid: 76.10% Test: 73.89%\n",
            "Epoch: 160, Loss: 0.4440, Train: 77.00%, Valid: 76.50% Test: 74.43%\n",
            "Epoch: 161, Loss: 0.4335, Train: 77.82%, Valid: 76.28% Test: 74.28%\n",
            "Epoch: 162, Loss: 0.4366, Train: 77.84%, Valid: 76.27% Test: 74.43%\n",
            "Epoch: 163, Loss: 0.4401, Train: 77.31%, Valid: 76.35% Test: 74.31%\n",
            "Epoch: 164, Loss: 0.4394, Train: 77.59%, Valid: 75.78% Test: 74.35%\n",
            "Epoch: 165, Loss: 0.4393, Train: 77.97%, Valid: 75.82% Test: 74.32%\n",
            "Epoch: 166, Loss: 0.4360, Train: 77.86%, Valid: 76.01% Test: 73.72%\n",
            "Epoch: 167, Loss: 0.4396, Train: 77.84%, Valid: 75.83% Test: 73.83%\n",
            "Epoch: 168, Loss: 0.4396, Train: 77.65%, Valid: 75.87% Test: 73.86%\n",
            "Epoch: 169, Loss: 0.4415, Train: 78.00%, Valid: 76.09% Test: 73.78%\n",
            "Epoch: 170, Loss: 0.4435, Train: 77.75%, Valid: 75.48% Test: 74.06%\n",
            "Epoch: 171, Loss: 0.4415, Train: 77.56%, Valid: 75.88% Test: 73.72%\n",
            "Epoch: 172, Loss: 0.4307, Train: 77.69%, Valid: 75.45% Test: 73.57%\n",
            "Epoch: 173, Loss: 0.4379, Train: 77.97%, Valid: 75.45% Test: 73.01%\n",
            "Epoch: 174, Loss: 0.4359, Train: 77.48%, Valid: 75.10% Test: 72.56%\n",
            "Epoch: 175, Loss: 0.4364, Train: 77.24%, Valid: 74.92% Test: 72.89%\n",
            "Epoch: 176, Loss: 0.4368, Train: 77.59%, Valid: 75.22% Test: 72.56%\n",
            "Epoch: 177, Loss: 0.4262, Train: 78.48%, Valid: 74.91% Test: 72.68%\n",
            "Epoch: 178, Loss: 0.4361, Train: 77.35%, Valid: 75.82% Test: 73.06%\n",
            "Epoch: 179, Loss: 0.4356, Train: 77.98%, Valid: 76.18% Test: 73.94%\n",
            "Epoch: 180, Loss: 0.4361, Train: 77.53%, Valid: 76.16% Test: 74.17%\n",
            "Epoch: 181, Loss: 0.4345, Train: 77.75%, Valid: 75.71% Test: 74.49%\n",
            "Epoch: 182, Loss: 0.4348, Train: 78.09%, Valid: 76.54% Test: 74.35%\n",
            "Epoch: 183, Loss: 0.4378, Train: 78.46%, Valid: 75.97% Test: 74.32%\n",
            "Epoch: 184, Loss: 0.4341, Train: 78.35%, Valid: 75.74% Test: 74.40%\n",
            "Epoch: 185, Loss: 0.4291, Train: 78.39%, Valid: 75.68% Test: 74.46%\n",
            "Epoch: 186, Loss: 0.4392, Train: 77.18%, Valid: 76.15% Test: 74.40%\n",
            "Epoch: 187, Loss: 0.4306, Train: 77.45%, Valid: 75.78% Test: 73.98%\n",
            "Epoch: 188, Loss: 0.4366, Train: 77.34%, Valid: 75.83% Test: 74.14%\n",
            "Epoch: 189, Loss: 0.4353, Train: 77.30%, Valid: 75.64% Test: 74.10%\n",
            "Epoch: 190, Loss: 0.4379, Train: 78.07%, Valid: 75.41% Test: 73.98%\n",
            "Epoch: 191, Loss: 0.4293, Train: 77.97%, Valid: 75.64% Test: 74.61%\n",
            "Epoch: 192, Loss: 0.4299, Train: 78.04%, Valid: 76.05% Test: 74.26%\n",
            "Epoch: 193, Loss: 0.4336, Train: 78.26%, Valid: 75.90% Test: 74.01%\n",
            "Epoch: 194, Loss: 0.4271, Train: 78.43%, Valid: 75.97% Test: 73.86%\n",
            "Epoch: 195, Loss: 0.4317, Train: 77.31%, Valid: 76.07% Test: 74.00%\n",
            "Epoch: 196, Loss: 0.4325, Train: 77.64%, Valid: 75.96% Test: 74.04%\n",
            "Epoch: 197, Loss: 0.4300, Train: 78.20%, Valid: 75.80% Test: 74.09%\n",
            "Epoch: 198, Loss: 0.4312, Train: 77.87%, Valid: 76.26% Test: 74.72%\n",
            "Epoch: 199, Loss: 0.4299, Train: 77.86%, Valid: 76.18% Test: 75.11%\n",
            "Epoch: 200, Loss: 0.4287, Train: 78.33%, Valid: 75.82% Test: 74.65%\n",
            "Epoch: 201, Loss: 0.4366, Train: 77.39%, Valid: 76.67% Test: 74.92%\n",
            "Epoch: 202, Loss: 0.4290, Train: 77.66%, Valid: 76.22% Test: 74.71%\n",
            "Epoch: 203, Loss: 0.4307, Train: 78.02%, Valid: 76.33% Test: 74.48%\n",
            "Epoch: 204, Loss: 0.4312, Train: 77.87%, Valid: 76.11% Test: 74.35%\n",
            "Epoch: 205, Loss: 0.4324, Train: 78.43%, Valid: 75.75% Test: 74.33%\n",
            "Epoch: 206, Loss: 0.4300, Train: 78.25%, Valid: 76.13% Test: 74.57%\n",
            "Epoch: 207, Loss: 0.4280, Train: 78.76%, Valid: 76.21% Test: 74.17%\n",
            "Epoch: 208, Loss: 0.4269, Train: 78.51%, Valid: 76.53% Test: 74.64%\n",
            "Epoch: 209, Loss: 0.4323, Train: 78.16%, Valid: 76.18% Test: 74.47%\n",
            "Epoch: 210, Loss: 0.4336, Train: 78.80%, Valid: 76.66% Test: 74.22%\n",
            "Epoch: 211, Loss: 0.4265, Train: 78.19%, Valid: 75.76% Test: 73.57%\n",
            "Epoch: 212, Loss: 0.4250, Train: 78.58%, Valid: 75.77% Test: 73.79%\n",
            "Epoch: 213, Loss: 0.4335, Train: 78.13%, Valid: 75.90% Test: 73.69%\n",
            "Epoch: 214, Loss: 0.4275, Train: 78.41%, Valid: 76.04% Test: 73.81%\n",
            "Epoch: 215, Loss: 0.4269, Train: 78.51%, Valid: 76.02% Test: 73.75%\n",
            "Epoch: 216, Loss: 0.4297, Train: 78.58%, Valid: 75.67% Test: 73.33%\n",
            "Epoch: 217, Loss: 0.4229, Train: 78.76%, Valid: 75.85% Test: 73.09%\n",
            "Epoch: 218, Loss: 0.4255, Train: 78.75%, Valid: 75.75% Test: 73.10%\n",
            "Epoch: 219, Loss: 0.4283, Train: 78.66%, Valid: 75.64% Test: 73.64%\n",
            "Epoch: 220, Loss: 0.4311, Train: 78.81%, Valid: 76.20% Test: 73.97%\n",
            "Epoch: 221, Loss: 0.4292, Train: 78.62%, Valid: 75.67% Test: 73.35%\n",
            "Epoch: 222, Loss: 0.4224, Train: 78.59%, Valid: 75.74% Test: 73.46%\n",
            "Epoch: 223, Loss: 0.4261, Train: 78.23%, Valid: 76.24% Test: 73.71%\n",
            "Epoch: 224, Loss: 0.4269, Train: 78.98%, Valid: 75.71% Test: 73.32%\n",
            "Epoch: 225, Loss: 0.4197, Train: 78.68%, Valid: 75.43% Test: 73.24%\n",
            "Epoch: 226, Loss: 0.4247, Train: 78.56%, Valid: 75.72% Test: 73.25%\n",
            "Epoch: 227, Loss: 0.4249, Train: 78.62%, Valid: 75.26% Test: 73.34%\n",
            "Epoch: 228, Loss: 0.4254, Train: 78.44%, Valid: 75.92% Test: 73.25%\n",
            "Epoch: 229, Loss: 0.4234, Train: 78.60%, Valid: 76.05% Test: 73.36%\n",
            "Epoch: 230, Loss: 0.4260, Train: 78.32%, Valid: 75.55% Test: 73.78%\n",
            "Epoch: 231, Loss: 0.4261, Train: 78.38%, Valid: 75.71% Test: 73.66%\n",
            "Epoch: 232, Loss: 0.4264, Train: 78.80%, Valid: 75.57% Test: 73.55%\n",
            "Epoch: 233, Loss: 0.4200, Train: 78.33%, Valid: 75.99% Test: 73.17%\n",
            "Epoch: 234, Loss: 0.4273, Train: 78.17%, Valid: 75.69% Test: 73.01%\n",
            "Epoch: 235, Loss: 0.4193, Train: 78.92%, Valid: 75.68% Test: 73.04%\n",
            "Epoch: 236, Loss: 0.4241, Train: 78.26%, Valid: 75.02% Test: 73.29%\n",
            "Epoch: 237, Loss: 0.4236, Train: 77.99%, Valid: 75.32% Test: 73.39%\n",
            "Epoch: 238, Loss: 0.4219, Train: 78.42%, Valid: 75.77% Test: 73.47%\n",
            "Epoch: 239, Loss: 0.4207, Train: 78.07%, Valid: 75.61% Test: 73.43%\n",
            "Epoch: 240, Loss: 0.4190, Train: 78.08%, Valid: 75.30% Test: 73.36%\n",
            "Epoch: 241, Loss: 0.4186, Train: 77.79%, Valid: 75.13% Test: 73.49%\n",
            "Epoch: 242, Loss: 0.4199, Train: 78.10%, Valid: 75.86% Test: 73.61%\n",
            "Epoch: 243, Loss: 0.4280, Train: 77.47%, Valid: 75.40% Test: 73.49%\n",
            "Epoch: 244, Loss: 0.4223, Train: 77.62%, Valid: 75.22% Test: 73.14%\n",
            "Epoch: 245, Loss: 0.4209, Train: 78.02%, Valid: 75.45% Test: 73.57%\n",
            "Epoch: 246, Loss: 0.4265, Train: 78.20%, Valid: 75.90% Test: 73.58%\n",
            "Epoch: 247, Loss: 0.4252, Train: 78.59%, Valid: 75.92% Test: 73.58%\n",
            "Epoch: 248, Loss: 0.4212, Train: 78.69%, Valid: 75.77% Test: 73.81%\n",
            "Epoch: 249, Loss: 0.4223, Train: 79.10%, Valid: 76.17% Test: 73.64%\n",
            "Epoch: 250, Loss: 0.4210, Train: 78.80%, Valid: 75.89% Test: 73.83%\n",
            "Epoch: 251, Loss: 0.4206, Train: 79.16%, Valid: 75.92% Test: 73.97%\n",
            "Epoch: 252, Loss: 0.4197, Train: 78.45%, Valid: 75.80% Test: 73.91%\n",
            "Epoch: 253, Loss: 0.4182, Train: 78.65%, Valid: 75.98% Test: 73.82%\n",
            "Epoch: 254, Loss: 0.4207, Train: 78.10%, Valid: 75.49% Test: 73.02%\n",
            "Epoch: 255, Loss: 0.4233, Train: 78.87%, Valid: 75.91% Test: 73.23%\n",
            "Epoch: 256, Loss: 0.4222, Train: 77.85%, Valid: 75.79% Test: 73.46%\n",
            "Epoch: 257, Loss: 0.4168, Train: 78.71%, Valid: 75.77% Test: 73.54%\n",
            "Epoch: 258, Loss: 0.4188, Train: 78.47%, Valid: 76.13% Test: 74.07%\n",
            "Epoch: 259, Loss: 0.4249, Train: 77.95%, Valid: 75.97% Test: 74.08%\n",
            "Epoch: 260, Loss: 0.4233, Train: 78.34%, Valid: 76.33% Test: 74.12%\n",
            "Epoch: 261, Loss: 0.4227, Train: 77.89%, Valid: 76.30% Test: 73.89%\n",
            "Epoch: 262, Loss: 0.4190, Train: 79.00%, Valid: 75.86% Test: 73.82%\n",
            "Epoch: 263, Loss: 0.4183, Train: 78.91%, Valid: 76.13% Test: 73.96%\n",
            "Epoch: 264, Loss: 0.4154, Train: 78.71%, Valid: 75.79% Test: 73.77%\n",
            "Epoch: 265, Loss: 0.4146, Train: 78.60%, Valid: 75.78% Test: 73.97%\n",
            "Epoch: 266, Loss: 0.4215, Train: 78.31%, Valid: 76.07% Test: 74.46%\n",
            "Epoch: 267, Loss: 0.4188, Train: 78.47%, Valid: 76.45% Test: 74.00%\n",
            "Epoch: 268, Loss: 0.4217, Train: 78.31%, Valid: 76.19% Test: 74.25%\n",
            "Epoch: 269, Loss: 0.4197, Train: 78.54%, Valid: 76.18% Test: 74.68%\n",
            "Epoch: 270, Loss: 0.4164, Train: 78.82%, Valid: 76.57% Test: 74.85%\n",
            "Epoch: 271, Loss: 0.4174, Train: 78.38%, Valid: 76.52% Test: 75.19%\n",
            "Epoch: 272, Loss: 0.4175, Train: 78.55%, Valid: 76.69% Test: 74.54%\n",
            "Epoch: 273, Loss: 0.4184, Train: 78.67%, Valid: 76.23% Test: 74.47%\n",
            "Epoch: 274, Loss: 0.4151, Train: 78.01%, Valid: 75.97% Test: 73.22%\n",
            "Epoch: 275, Loss: 0.4119, Train: 78.50%, Valid: 75.37% Test: 72.65%\n",
            "Epoch: 276, Loss: 0.4181, Train: 78.39%, Valid: 75.48% Test: 73.07%\n",
            "Epoch: 277, Loss: 0.4174, Train: 78.00%, Valid: 75.41% Test: 73.51%\n",
            "Epoch: 278, Loss: 0.4162, Train: 78.25%, Valid: 75.37% Test: 73.81%\n",
            "Epoch: 279, Loss: 0.4136, Train: 78.57%, Valid: 75.87% Test: 74.16%\n",
            "Epoch: 280, Loss: 0.4178, Train: 78.60%, Valid: 76.46% Test: 74.38%\n",
            "Epoch: 281, Loss: 0.4167, Train: 78.37%, Valid: 75.80% Test: 74.00%\n",
            "Epoch: 282, Loss: 0.4095, Train: 79.14%, Valid: 76.33% Test: 74.01%\n",
            "Epoch: 283, Loss: 0.4097, Train: 79.16%, Valid: 76.00% Test: 74.65%\n",
            "Epoch: 284, Loss: 0.4160, Train: 78.78%, Valid: 76.28% Test: 74.30%\n",
            "Epoch: 285, Loss: 0.4079, Train: 78.92%, Valid: 76.08% Test: 74.09%\n",
            "Epoch: 286, Loss: 0.4177, Train: 78.59%, Valid: 75.59% Test: 73.93%\n",
            "Epoch: 287, Loss: 0.4116, Train: 78.63%, Valid: 75.71% Test: 73.18%\n",
            "Epoch: 288, Loss: 0.4100, Train: 78.85%, Valid: 75.92% Test: 73.27%\n",
            "Epoch: 289, Loss: 0.4161, Train: 79.16%, Valid: 75.87% Test: 73.12%\n",
            "Epoch: 290, Loss: 0.4154, Train: 78.68%, Valid: 75.25% Test: 73.03%\n",
            "Epoch: 291, Loss: 0.4099, Train: 78.98%, Valid: 75.61% Test: 72.36%\n",
            "Epoch: 292, Loss: 0.4111, Train: 78.80%, Valid: 75.02% Test: 71.90%\n",
            "Epoch: 293, Loss: 0.4126, Train: 78.50%, Valid: 74.75% Test: 71.99%\n",
            "Epoch: 294, Loss: 0.4177, Train: 78.93%, Valid: 74.52% Test: 71.49%\n",
            "Epoch: 295, Loss: 0.4109, Train: 78.36%, Valid: 73.26% Test: 70.89%\n",
            "Epoch: 296, Loss: 0.4188, Train: 78.53%, Valid: 74.07% Test: 71.12%\n",
            "Epoch: 297, Loss: 0.4147, Train: 78.07%, Valid: 74.82% Test: 71.77%\n",
            "Epoch: 298, Loss: 0.4102, Train: 79.06%, Valid: 75.29% Test: 73.00%\n",
            "Epoch: 299, Loss: 0.4104, Train: 78.40%, Valid: 75.56% Test: 73.82%\n",
            "Epoch: 300, Loss: 0.4150, Train: 78.49%, Valid: 76.05% Test: 73.74%\n",
            "Epoch: 301, Loss: 0.4084, Train: 78.65%, Valid: 75.66% Test: 73.64%\n",
            "Epoch: 302, Loss: 0.4112, Train: 78.56%, Valid: 75.57% Test: 73.64%\n",
            "Epoch: 303, Loss: 0.4041, Train: 78.49%, Valid: 75.70% Test: 73.60%\n",
            "Epoch: 304, Loss: 0.4082, Train: 78.36%, Valid: 75.99% Test: 74.04%\n",
            "Epoch: 305, Loss: 0.4127, Train: 77.88%, Valid: 75.38% Test: 74.22%\n",
            "Epoch: 306, Loss: 0.4108, Train: 77.85%, Valid: 75.33% Test: 73.87%\n",
            "Epoch: 307, Loss: 0.4102, Train: 77.90%, Valid: 74.89% Test: 73.72%\n",
            "Epoch: 308, Loss: 0.4099, Train: 77.78%, Valid: 75.00% Test: 73.64%\n",
            "Epoch: 309, Loss: 0.4110, Train: 77.10%, Valid: 75.39% Test: 73.81%\n",
            "Epoch: 310, Loss: 0.4064, Train: 78.05%, Valid: 74.88% Test: 73.85%\n",
            "Epoch: 311, Loss: 0.4132, Train: 76.82%, Valid: 75.11% Test: 73.35%\n",
            "Epoch: 312, Loss: 0.4092, Train: 77.72%, Valid: 74.33% Test: 73.53%\n",
            "Epoch: 313, Loss: 0.4123, Train: 76.69%, Valid: 74.60% Test: 73.03%\n",
            "Epoch: 314, Loss: 0.4110, Train: 77.24%, Valid: 74.99% Test: 72.73%\n",
            "Epoch: 315, Loss: 0.4083, Train: 78.03%, Valid: 74.53% Test: 72.64%\n",
            "Epoch: 316, Loss: 0.4085, Train: 77.65%, Valid: 74.78% Test: 72.82%\n",
            "Epoch: 317, Loss: 0.4080, Train: 78.03%, Valid: 75.17% Test: 72.72%\n",
            "Epoch: 318, Loss: 0.4031, Train: 78.56%, Valid: 75.17% Test: 73.31%\n",
            "Epoch: 319, Loss: 0.4077, Train: 78.66%, Valid: 75.71% Test: 73.17%\n",
            "Epoch: 320, Loss: 0.4086, Train: 77.83%, Valid: 75.36% Test: 73.13%\n",
            "Epoch: 321, Loss: 0.4102, Train: 77.33%, Valid: 74.77% Test: 73.05%\n",
            "Epoch: 322, Loss: 0.4077, Train: 77.81%, Valid: 75.52% Test: 73.14%\n",
            "Epoch: 323, Loss: 0.4073, Train: 77.69%, Valid: 75.37% Test: 73.74%\n",
            "Epoch: 324, Loss: 0.4051, Train: 78.53%, Valid: 75.27% Test: 73.53%\n",
            "Epoch: 325, Loss: 0.4058, Train: 78.68%, Valid: 75.77% Test: 73.75%\n",
            "Epoch: 326, Loss: 0.4083, Train: 77.99%, Valid: 75.84% Test: 74.08%\n",
            "Epoch: 327, Loss: 0.4063, Train: 78.25%, Valid: 76.43% Test: 73.98%\n",
            "Epoch: 328, Loss: 0.4072, Train: 79.20%, Valid: 76.38% Test: 74.06%\n",
            "Epoch: 329, Loss: 0.4068, Train: 79.35%, Valid: 75.79% Test: 73.75%\n",
            "Epoch: 330, Loss: 0.4096, Train: 79.11%, Valid: 75.50% Test: 73.43%\n",
            "Epoch: 331, Loss: 0.4078, Train: 79.48%, Valid: 75.35% Test: 72.85%\n",
            "Epoch: 332, Loss: 0.4055, Train: 79.11%, Valid: 75.50% Test: 72.69%\n",
            "Epoch: 333, Loss: 0.4041, Train: 78.80%, Valid: 75.93% Test: 72.96%\n",
            "Epoch: 334, Loss: 0.4051, Train: 79.19%, Valid: 74.84% Test: 71.89%\n",
            "Epoch: 335, Loss: 0.4097, Train: 78.38%, Valid: 74.81% Test: 71.87%\n",
            "Epoch: 336, Loss: 0.4051, Train: 78.76%, Valid: 75.22% Test: 72.53%\n",
            "Epoch: 337, Loss: 0.4045, Train: 78.95%, Valid: 75.23% Test: 73.01%\n",
            "Epoch: 338, Loss: 0.4064, Train: 78.22%, Valid: 75.70% Test: 73.26%\n",
            "Epoch: 339, Loss: 0.4036, Train: 79.36%, Valid: 75.60% Test: 73.63%\n",
            "Epoch: 340, Loss: 0.4095, Train: 78.47%, Valid: 74.98% Test: 72.97%\n",
            "Epoch: 341, Loss: 0.4082, Train: 78.30%, Valid: 75.06% Test: 72.09%\n",
            "Epoch: 342, Loss: 0.4003, Train: 79.40%, Valid: 75.48% Test: 72.32%\n",
            "Epoch: 343, Loss: 0.4076, Train: 78.70%, Valid: 75.19% Test: 73.11%\n",
            "Epoch: 344, Loss: 0.4071, Train: 79.28%, Valid: 76.11% Test: 73.72%\n",
            "Epoch: 345, Loss: 0.3989, Train: 79.10%, Valid: 75.67% Test: 73.64%\n",
            "Epoch: 346, Loss: 0.4057, Train: 78.80%, Valid: 76.18% Test: 73.58%\n",
            "Epoch: 347, Loss: 0.4078, Train: 79.34%, Valid: 76.31% Test: 73.67%\n",
            "Epoch: 348, Loss: 0.4025, Train: 79.10%, Valid: 75.86% Test: 73.81%\n",
            "Epoch: 349, Loss: 0.4035, Train: 78.97%, Valid: 76.41% Test: 74.25%\n",
            "Epoch: 350, Loss: 0.4040, Train: 78.77%, Valid: 76.43% Test: 74.08%\n",
            "Epoch: 351, Loss: 0.4074, Train: 78.55%, Valid: 75.76% Test: 74.07%\n",
            "Epoch: 352, Loss: 0.4039, Train: 79.70%, Valid: 76.32% Test: 73.71%\n",
            "Epoch: 353, Loss: 0.4059, Train: 78.94%, Valid: 76.02% Test: 74.09%\n",
            "Epoch: 354, Loss: 0.3962, Train: 78.50%, Valid: 76.09% Test: 73.81%\n",
            "Epoch: 355, Loss: 0.4031, Train: 78.78%, Valid: 75.71% Test: 73.01%\n",
            "Epoch: 356, Loss: 0.4052, Train: 78.50%, Valid: 74.73% Test: 72.44%\n",
            "Epoch: 357, Loss: 0.4030, Train: 77.96%, Valid: 74.30% Test: 72.10%\n",
            "Epoch: 358, Loss: 0.4046, Train: 77.62%, Valid: 73.51% Test: 70.86%\n",
            "Epoch: 359, Loss: 0.4014, Train: 77.94%, Valid: 73.52% Test: 70.73%\n",
            "Epoch: 360, Loss: 0.4051, Train: 77.30%, Valid: 73.12% Test: 70.53%\n",
            "Epoch: 361, Loss: 0.4024, Train: 76.72%, Valid: 73.12% Test: 70.15%\n",
            "Epoch: 362, Loss: 0.4064, Train: 77.21%, Valid: 72.93% Test: 70.38%\n",
            "Epoch: 363, Loss: 0.4022, Train: 77.21%, Valid: 73.58% Test: 70.83%\n",
            "Epoch: 364, Loss: 0.4038, Train: 78.40%, Valid: 73.82% Test: 71.09%\n",
            "Epoch: 365, Loss: 0.3994, Train: 77.96%, Valid: 74.46% Test: 71.60%\n",
            "Epoch: 366, Loss: 0.4031, Train: 78.53%, Valid: 74.77% Test: 71.79%\n",
            "Epoch: 367, Loss: 0.4011, Train: 79.28%, Valid: 74.98% Test: 71.83%\n",
            "Epoch: 368, Loss: 0.4052, Train: 79.22%, Valid: 75.26% Test: 72.31%\n",
            "Epoch: 369, Loss: 0.4010, Train: 79.28%, Valid: 75.52% Test: 72.64%\n",
            "Epoch: 370, Loss: 0.4039, Train: 78.69%, Valid: 75.80% Test: 72.97%\n",
            "Epoch: 371, Loss: 0.4030, Train: 78.44%, Valid: 75.81% Test: 72.53%\n",
            "Epoch: 372, Loss: 0.3926, Train: 78.87%, Valid: 75.29% Test: 72.78%\n",
            "Epoch: 373, Loss: 0.4043, Train: 78.35%, Valid: 75.85% Test: 72.99%\n",
            "Epoch: 374, Loss: 0.4006, Train: 78.87%, Valid: 75.56% Test: 72.90%\n",
            "Epoch: 375, Loss: 0.3947, Train: 79.23%, Valid: 75.52% Test: 73.08%\n",
            "Epoch: 376, Loss: 0.3959, Train: 78.22%, Valid: 75.90% Test: 73.60%\n",
            "Epoch: 377, Loss: 0.4027, Train: 77.89%, Valid: 75.62% Test: 73.95%\n",
            "Epoch: 378, Loss: 0.4021, Train: 78.58%, Valid: 76.43% Test: 74.19%\n",
            "Epoch: 379, Loss: 0.4030, Train: 79.07%, Valid: 76.82% Test: 74.45%\n",
            "Epoch: 380, Loss: 0.3996, Train: 79.46%, Valid: 76.69% Test: 73.56%\n",
            "Epoch: 381, Loss: 0.4074, Train: 78.79%, Valid: 76.35% Test: 73.32%\n",
            "Epoch: 382, Loss: 0.3956, Train: 79.53%, Valid: 75.73% Test: 73.28%\n",
            "Epoch: 383, Loss: 0.4042, Train: 79.04%, Valid: 76.60% Test: 73.57%\n",
            "Epoch: 384, Loss: 0.3978, Train: 78.68%, Valid: 75.97% Test: 73.32%\n",
            "Epoch: 385, Loss: 0.4029, Train: 78.36%, Valid: 76.04% Test: 73.34%\n",
            "Epoch: 386, Loss: 0.4037, Train: 78.66%, Valid: 76.01% Test: 73.44%\n",
            "Epoch: 387, Loss: 0.3998, Train: 78.56%, Valid: 75.85% Test: 73.92%\n",
            "Epoch: 388, Loss: 0.3981, Train: 78.21%, Valid: 76.07% Test: 74.25%\n",
            "Epoch: 389, Loss: 0.4003, Train: 78.80%, Valid: 76.55% Test: 74.93%\n",
            "Epoch: 390, Loss: 0.4003, Train: 78.51%, Valid: 76.84% Test: 75.15%\n",
            "Epoch: 391, Loss: 0.4019, Train: 78.59%, Valid: 77.40% Test: 74.76%\n",
            "Epoch: 392, Loss: 0.3953, Train: 79.38%, Valid: 76.73% Test: 74.54%\n",
            "Epoch: 393, Loss: 0.3943, Train: 79.27%, Valid: 76.77% Test: 75.01%\n",
            "Epoch: 394, Loss: 0.3989, Train: 78.56%, Valid: 76.49% Test: 74.96%\n",
            "Epoch: 395, Loss: 0.3936, Train: 79.31%, Valid: 76.73% Test: 75.21%\n",
            "Epoch: 396, Loss: 0.3984, Train: 78.38%, Valid: 76.78% Test: 75.22%\n",
            "Epoch: 397, Loss: 0.3997, Train: 78.49%, Valid: 75.98% Test: 74.78%\n",
            "Epoch: 398, Loss: 0.3999, Train: 79.31%, Valid: 75.99% Test: 74.21%\n",
            "Epoch: 399, Loss: 0.3897, Train: 79.63%, Valid: 75.53% Test: 73.40%\n",
            "Epoch: 400, Loss: 0.3952, Train: 79.30%, Valid: 76.03% Test: 73.42%\n",
            "Epoch: 401, Loss: 0.3983, Train: 79.31%, Valid: 76.68% Test: 74.07%\n",
            "Epoch: 402, Loss: 0.3919, Train: 79.11%, Valid: 76.48% Test: 74.52%\n",
            "Epoch: 403, Loss: 0.3950, Train: 78.47%, Valid: 76.54% Test: 74.32%\n",
            "Epoch: 404, Loss: 0.3958, Train: 78.32%, Valid: 76.53% Test: 74.39%\n",
            "Epoch: 405, Loss: 0.3940, Train: 78.57%, Valid: 76.17% Test: 73.96%\n",
            "Epoch: 406, Loss: 0.3997, Train: 78.56%, Valid: 76.07% Test: 74.83%\n",
            "Epoch: 407, Loss: 0.3937, Train: 78.22%, Valid: 76.05% Test: 74.97%\n",
            "Epoch: 408, Loss: 0.3957, Train: 77.94%, Valid: 76.60% Test: 74.58%\n",
            "Epoch: 409, Loss: 0.3989, Train: 78.53%, Valid: 76.28% Test: 74.58%\n",
            "Epoch: 410, Loss: 0.3966, Train: 78.61%, Valid: 76.68% Test: 74.33%\n",
            "Epoch: 411, Loss: 0.4017, Train: 78.97%, Valid: 75.92% Test: 73.75%\n",
            "Epoch: 412, Loss: 0.3957, Train: 79.30%, Valid: 76.47% Test: 73.71%\n",
            "Epoch: 413, Loss: 0.3965, Train: 78.86%, Valid: 75.90% Test: 73.85%\n",
            "Epoch: 414, Loss: 0.3919, Train: 78.08%, Valid: 76.05% Test: 73.42%\n",
            "Epoch: 415, Loss: 0.3958, Train: 78.56%, Valid: 76.04% Test: 73.65%\n",
            "Epoch: 416, Loss: 0.3995, Train: 79.11%, Valid: 76.32% Test: 73.60%\n",
            "Epoch: 417, Loss: 0.3936, Train: 79.27%, Valid: 76.40% Test: 73.03%\n",
            "Epoch: 418, Loss: 0.3965, Train: 79.01%, Valid: 75.86% Test: 73.42%\n",
            "Epoch: 419, Loss: 0.3915, Train: 78.47%, Valid: 76.15% Test: 73.94%\n",
            "Epoch: 420, Loss: 0.3940, Train: 78.18%, Valid: 76.43% Test: 73.66%\n",
            "Epoch: 421, Loss: 0.3960, Train: 78.24%, Valid: 75.80% Test: 73.74%\n",
            "Epoch: 422, Loss: 0.3957, Train: 79.20%, Valid: 75.86% Test: 73.21%\n",
            "Epoch: 423, Loss: 0.3868, Train: 79.08%, Valid: 75.64% Test: 72.76%\n",
            "Epoch: 424, Loss: 0.3933, Train: 79.10%, Valid: 75.93% Test: 72.95%\n",
            "Epoch: 425, Loss: 0.3968, Train: 79.24%, Valid: 76.26% Test: 72.96%\n",
            "Epoch: 426, Loss: 0.3957, Train: 79.26%, Valid: 76.08% Test: 72.96%\n",
            "Epoch: 427, Loss: 0.3894, Train: 79.30%, Valid: 75.32% Test: 72.49%\n",
            "Epoch: 428, Loss: 0.3939, Train: 79.49%, Valid: 74.96% Test: 72.06%\n",
            "Epoch: 429, Loss: 0.3932, Train: 79.52%, Valid: 75.59% Test: 72.56%\n",
            "Epoch: 430, Loss: 0.3958, Train: 78.96%, Valid: 75.79% Test: 73.05%\n",
            "Epoch: 431, Loss: 0.3975, Train: 78.32%, Valid: 75.66% Test: 72.60%\n",
            "Epoch: 432, Loss: 0.3915, Train: 78.32%, Valid: 74.80% Test: 71.99%\n",
            "Epoch: 433, Loss: 0.3907, Train: 77.94%, Valid: 74.37% Test: 71.51%\n",
            "Epoch: 434, Loss: 0.3899, Train: 77.70%, Valid: 74.52% Test: 72.25%\n",
            "Epoch: 435, Loss: 0.3937, Train: 78.47%, Valid: 74.83% Test: 72.72%\n",
            "Epoch: 436, Loss: 0.3969, Train: 78.13%, Valid: 75.04% Test: 73.14%\n",
            "Epoch: 437, Loss: 0.3901, Train: 78.82%, Valid: 75.59% Test: 73.28%\n",
            "Epoch: 438, Loss: 0.3903, Train: 78.74%, Valid: 75.36% Test: 72.92%\n",
            "Epoch: 439, Loss: 0.3923, Train: 78.92%, Valid: 74.79% Test: 72.24%\n",
            "Epoch: 440, Loss: 0.3960, Train: 78.61%, Valid: 73.72% Test: 71.39%\n",
            "Epoch: 441, Loss: 0.3929, Train: 78.68%, Valid: 73.79% Test: 70.83%\n",
            "Epoch: 442, Loss: 0.3893, Train: 78.53%, Valid: 74.39% Test: 71.26%\n",
            "Epoch: 443, Loss: 0.3963, Train: 79.49%, Valid: 74.87% Test: 72.13%\n",
            "Epoch: 444, Loss: 0.3940, Train: 78.86%, Valid: 75.15% Test: 72.03%\n",
            "Epoch: 445, Loss: 0.3861, Train: 79.85%, Valid: 75.75% Test: 72.92%\n",
            "Epoch: 446, Loss: 0.3954, Train: 78.82%, Valid: 75.19% Test: 73.19%\n",
            "Epoch: 447, Loss: 0.3896, Train: 77.72%, Valid: 75.72% Test: 73.33%\n",
            "Epoch: 448, Loss: 0.3937, Train: 76.24%, Valid: 74.69% Test: 72.93%\n",
            "Epoch: 449, Loss: 0.3957, Train: 75.50%, Valid: 74.39% Test: 73.31%\n",
            "Epoch: 450, Loss: 0.3950, Train: 76.59%, Valid: 74.77% Test: 73.54%\n",
            "Epoch: 451, Loss: 0.3980, Train: 76.52%, Valid: 74.95% Test: 74.04%\n",
            "Epoch: 452, Loss: 0.3892, Train: 77.37%, Valid: 75.53% Test: 74.35%\n",
            "Epoch: 453, Loss: 0.3915, Train: 78.38%, Valid: 75.85% Test: 74.66%\n",
            "Epoch: 454, Loss: 0.3894, Train: 78.50%, Valid: 76.95% Test: 74.75%\n",
            "Epoch: 455, Loss: 0.3885, Train: 79.74%, Valid: 76.42% Test: 74.02%\n",
            "Epoch: 456, Loss: 0.3962, Train: 79.11%, Valid: 75.50% Test: 72.84%\n",
            "Epoch: 457, Loss: 0.3958, Train: 78.91%, Valid: 74.27% Test: 71.25%\n",
            "Epoch: 458, Loss: 0.3896, Train: 79.00%, Valid: 73.89% Test: 71.37%\n",
            "Epoch: 459, Loss: 0.3878, Train: 79.07%, Valid: 75.00% Test: 72.35%\n",
            "Epoch: 460, Loss: 0.3872, Train: 78.67%, Valid: 74.77% Test: 72.60%\n",
            "Epoch: 461, Loss: 0.3904, Train: 78.24%, Valid: 74.82% Test: 72.50%\n",
            "Epoch: 462, Loss: 0.3884, Train: 78.28%, Valid: 74.37% Test: 72.53%\n",
            "Epoch: 463, Loss: 0.3879, Train: 77.91%, Valid: 75.02% Test: 71.96%\n",
            "Epoch: 464, Loss: 0.3875, Train: 76.58%, Valid: 74.07% Test: 72.28%\n",
            "Epoch: 465, Loss: 0.3852, Train: 76.14%, Valid: 73.05% Test: 72.12%\n",
            "Epoch: 466, Loss: 0.3954, Train: 75.62%, Valid: 73.69% Test: 72.36%\n",
            "Epoch: 467, Loss: 0.3886, Train: 77.03%, Valid: 74.34% Test: 72.93%\n",
            "Epoch: 468, Loss: 0.3882, Train: 77.60%, Valid: 74.58% Test: 73.24%\n",
            "Epoch: 469, Loss: 0.3863, Train: 78.05%, Valid: 74.64% Test: 72.81%\n",
            "Epoch: 470, Loss: 0.3934, Train: 77.63%, Valid: 74.61% Test: 72.65%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-86090fe8c0cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                               torch.zeros(valid_negative_samples.shape[0])))\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalid_eval_outfits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_negative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     valid_acc = test(model, x, valid_message_passing_edges.to(device)\n\u001b[0m\u001b[1;32m     27\u001b[0m     , valid_eval_outfits.to(device), valid_labels.to(device))\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4535d83d6ded>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, x, edge_index, outfits, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0membds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mo_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mo_pred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mo_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## results:\n",
        "We achived the best test result of 75.22 in epoch 396. In addition, we think that we will achieve better results, if we use a stronger approach for negative sampling, like using compatiblity dataset that is available.\n",
        "\n",
        "in the next section, we will train our model on a dataset with better negative samples, but also the same positive ones."
      ],
      "metadata": {
        "id": "2qvm4HEY7FTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Better negative samples"
      ],
      "metadata": {
        "id": "pe2RInbmH62r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gvK-9q6Z1UId"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "Tvdc5sQNACh1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_parameters()\n",
        "\n",
        "losses = []\n",
        "test_accs = []\n",
        "\n",
        "best_test_acc = 0\n",
        "best_model = None\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "x = x.to(device)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    train_positives_eval_outfits, message_passing_edges = split_train_data(train_edge_index, 5)\n",
        "    train_negatives_eval_outfits = negative_train_edge_index[torch.randperm(negative_train_edge_index.shape[0])[:train_positives_eval_outfits.shape[0]]]\n",
        "    train_labels = torch.cat((torch.ones(train_positives_eval_outfits.shape[0]),\n",
        "                              torch.zeros(train_negatives_eval_outfits.shape[0])))\n",
        "    train_eval_outfits = torch.cat((train_positives_eval_outfits, train_negatives_eval_outfits), dim=0)\n",
        "    train_loss = train(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device),\n",
        "                       loss_fn=loss_fn, optimizer=optimizer)\n",
        "    train_acc = test(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device))\n",
        "\n",
        "\n",
        "    valid_message_passing_edges = torch.cat((concat_edge_indices(train_positives_eval_outfits)\n",
        "    , message_passing_edges), dim=-1)\n",
        "    valid_labels = torch.cat((torch.ones(valid_edge_index.shape[0]),\n",
        "                              torch.zeros(valid_negative_samples.shape[0])))\n",
        "    valid_eval_outfits = torch.cat((valid_edge_index, valid_negative_samples), dim=0)\n",
        "    valid_acc = test(model, x, valid_message_passing_edges.to(device)\n",
        "    , valid_eval_outfits.to(device), valid_labels.to(device))\n",
        "\n",
        "\n",
        "\n",
        "    test_message_passing_edges = torch.cat((concat_edge_indices(valid_edge_index)\n",
        "    , valid_message_passing_edges), dim=-1)\n",
        "    test_labels = torch.cat((torch.ones(test_edge_index.shape[0]),\n",
        "                              torch.zeros(test_negative_samples.shape[0])))\n",
        "    test_eval_outfits = torch.cat((test_edge_index, test_negative_samples), dim=0)\n",
        "    test_acc = test(model, x, test_message_passing_edges.to(device)\n",
        "    , test_eval_outfits.to(device), test_labels.to(device))\n",
        "\n",
        "    if(test_acc > best_test_acc):\n",
        "        best_test_acc = test_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    losses.append(train_loss)\n",
        "    test_accs.append(test_acc)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {train_loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        ""
      ],
      "metadata": {
        "id": "15Aqxk3aH_et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db0f0cd-6169-40c7-a792-417094a7b6c6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 5.8179, Train: 50.00%, Valid: 50.00% Test: 49.99%\n",
            "Epoch: 01, Loss: 20.0343, Train: 49.94%, Valid: 49.95% Test: 49.93%\n",
            "Epoch: 02, Loss: 2.9330, Train: 49.93%, Valid: 49.89% Test: 49.92%\n",
            "Epoch: 03, Loss: 3.4506, Train: 49.92%, Valid: 49.97% Test: 49.98%\n",
            "Epoch: 04, Loss: 2.5571, Train: 49.95%, Valid: 49.94% Test: 49.95%\n",
            "Epoch: 05, Loss: 1.8593, Train: 49.94%, Valid: 49.95% Test: 49.95%\n",
            "Epoch: 06, Loss: 1.5677, Train: 49.95%, Valid: 49.96% Test: 49.96%\n",
            "Epoch: 07, Loss: 1.5298, Train: 49.94%, Valid: 49.99% Test: 49.97%\n",
            "Epoch: 08, Loss: 1.4849, Train: 49.95%, Valid: 49.94% Test: 49.94%\n",
            "Epoch: 09, Loss: 1.3183, Train: 49.93%, Valid: 49.92% Test: 49.94%\n",
            "Epoch: 10, Loss: 1.2356, Train: 49.93%, Valid: 49.90% Test: 49.93%\n",
            "Epoch: 11, Loss: 1.0900, Train: 49.90%, Valid: 49.80% Test: 49.91%\n",
            "Epoch: 12, Loss: 1.0993, Train: 49.85%, Valid: 49.91% Test: 49.90%\n",
            "Epoch: 13, Loss: 1.0905, Train: 49.88%, Valid: 49.83% Test: 49.88%\n",
            "Epoch: 14, Loss: 0.9625, Train: 49.85%, Valid: 49.84% Test: 49.88%\n",
            "Epoch: 15, Loss: 0.9579, Train: 49.84%, Valid: 49.84% Test: 49.86%\n",
            "Epoch: 16, Loss: 1.0139, Train: 49.81%, Valid: 49.75% Test: 49.78%\n",
            "Epoch: 17, Loss: 0.9956, Train: 49.74%, Valid: 49.80% Test: 49.85%\n",
            "Epoch: 18, Loss: 0.9369, Train: 49.75%, Valid: 49.76% Test: 49.71%\n",
            "Epoch: 19, Loss: 0.9011, Train: 49.79%, Valid: 49.67% Test: 49.77%\n",
            "Epoch: 20, Loss: 0.8696, Train: 49.80%, Valid: 49.71% Test: 49.75%\n",
            "Epoch: 21, Loss: 0.8109, Train: 49.82%, Valid: 49.64% Test: 49.58%\n",
            "Epoch: 22, Loss: 0.8176, Train: 49.81%, Valid: 49.66% Test: 49.72%\n",
            "Epoch: 23, Loss: 0.8762, Train: 49.68%, Valid: 49.80% Test: 49.65%\n",
            "Epoch: 24, Loss: 0.8362, Train: 49.79%, Valid: 49.60% Test: 49.54%\n",
            "Epoch: 25, Loss: 0.8473, Train: 49.82%, Valid: 49.59% Test: 49.50%\n",
            "Epoch: 26, Loss: 0.8119, Train: 49.79%, Valid: 49.48% Test: 49.48%\n",
            "Epoch: 27, Loss: 0.7988, Train: 49.86%, Valid: 49.46% Test: 49.40%\n",
            "Epoch: 28, Loss: 0.7437, Train: 50.17%, Valid: 49.47% Test: 49.67%\n",
            "Epoch: 29, Loss: 0.7765, Train: 50.53%, Valid: 49.83% Test: 49.75%\n",
            "Epoch: 30, Loss: 0.7641, Train: 50.98%, Valid: 50.29% Test: 50.31%\n",
            "Epoch: 31, Loss: 0.7389, Train: 51.37%, Valid: 51.02% Test: 50.98%\n",
            "Epoch: 32, Loss: 0.7896, Train: 52.37%, Valid: 51.84% Test: 51.49%\n",
            "Epoch: 33, Loss: 0.7536, Train: 52.91%, Valid: 53.20% Test: 52.40%\n",
            "Epoch: 34, Loss: 0.7186, Train: 54.68%, Valid: 54.67% Test: 53.92%\n",
            "Epoch: 35, Loss: 0.7338, Train: 55.47%, Valid: 56.48% Test: 55.51%\n",
            "Epoch: 36, Loss: 0.7074, Train: 56.51%, Valid: 57.20% Test: 57.16%\n",
            "Epoch: 37, Loss: 0.6776, Train: 58.00%, Valid: 59.13% Test: 58.39%\n",
            "Epoch: 38, Loss: 0.7028, Train: 58.90%, Valid: 60.15% Test: 59.90%\n",
            "Epoch: 39, Loss: 0.6827, Train: 60.19%, Valid: 62.10% Test: 61.21%\n",
            "Epoch: 40, Loss: 0.6664, Train: 61.21%, Valid: 62.44% Test: 62.29%\n",
            "Epoch: 41, Loss: 0.6778, Train: 62.03%, Valid: 63.17% Test: 63.19%\n",
            "Epoch: 42, Loss: 0.6932, Train: 62.66%, Valid: 64.06% Test: 64.29%\n",
            "Epoch: 43, Loss: 0.6540, Train: 63.40%, Valid: 64.52% Test: 64.66%\n",
            "Epoch: 44, Loss: 0.7058, Train: 63.67%, Valid: 65.42% Test: 65.17%\n",
            "Epoch: 45, Loss: 0.6731, Train: 64.45%, Valid: 65.93% Test: 66.06%\n",
            "Epoch: 46, Loss: 0.6706, Train: 64.61%, Valid: 65.77% Test: 65.74%\n",
            "Epoch: 47, Loss: 0.6616, Train: 65.27%, Valid: 66.00% Test: 66.33%\n",
            "Epoch: 48, Loss: 0.6425, Train: 64.73%, Valid: 66.07% Test: 66.53%\n",
            "Epoch: 49, Loss: 0.6422, Train: 65.20%, Valid: 66.66% Test: 67.07%\n",
            "Epoch: 50, Loss: 0.6083, Train: 65.48%, Valid: 67.18% Test: 66.95%\n",
            "Epoch: 51, Loss: 0.6409, Train: 65.66%, Valid: 66.75% Test: 66.65%\n",
            "Epoch: 52, Loss: 0.6316, Train: 66.13%, Valid: 66.78% Test: 67.27%\n",
            "Epoch: 53, Loss: 0.6054, Train: 66.19%, Valid: 67.18% Test: 67.53%\n",
            "Epoch: 54, Loss: 0.6230, Train: 66.14%, Valid: 67.21% Test: 67.79%\n",
            "Epoch: 55, Loss: 0.6262, Train: 66.25%, Valid: 67.19% Test: 67.87%\n",
            "Epoch: 56, Loss: 0.6152, Train: 66.52%, Valid: 67.84% Test: 68.04%\n",
            "Epoch: 57, Loss: 0.6117, Train: 66.41%, Valid: 68.48% Test: 67.78%\n",
            "Epoch: 58, Loss: 0.6112, Train: 66.34%, Valid: 67.87% Test: 68.33%\n",
            "Epoch: 59, Loss: 0.6654, Train: 65.95%, Valid: 67.47% Test: 68.18%\n",
            "Epoch: 60, Loss: 0.6128, Train: 66.36%, Valid: 68.73% Test: 67.89%\n",
            "Epoch: 61, Loss: 0.5919, Train: 66.50%, Valid: 67.62% Test: 68.32%\n",
            "Epoch: 62, Loss: 0.6093, Train: 67.52%, Valid: 68.45% Test: 68.50%\n",
            "Epoch: 63, Loss: 0.6070, Train: 66.85%, Valid: 68.20% Test: 68.70%\n",
            "Epoch: 64, Loss: 0.6293, Train: 66.99%, Valid: 68.34% Test: 68.79%\n",
            "Epoch: 65, Loss: 0.5986, Train: 67.58%, Valid: 68.18% Test: 68.71%\n",
            "Epoch: 66, Loss: 0.5780, Train: 67.72%, Valid: 68.07% Test: 68.57%\n",
            "Epoch: 67, Loss: 0.5695, Train: 67.70%, Valid: 68.60% Test: 68.60%\n",
            "Epoch: 68, Loss: 0.5921, Train: 67.36%, Valid: 68.55% Test: 68.93%\n",
            "Epoch: 69, Loss: 0.5883, Train: 68.34%, Valid: 68.88% Test: 68.88%\n",
            "Epoch: 70, Loss: 0.5850, Train: 68.21%, Valid: 68.67% Test: 69.12%\n",
            "Epoch: 71, Loss: 0.5686, Train: 68.26%, Valid: 69.37% Test: 69.22%\n",
            "Epoch: 72, Loss: 0.5765, Train: 68.18%, Valid: 68.93% Test: 69.25%\n",
            "Epoch: 73, Loss: 0.6024, Train: 68.19%, Valid: 69.19% Test: 69.21%\n",
            "Epoch: 74, Loss: 0.5650, Train: 68.78%, Valid: 68.70% Test: 68.79%\n",
            "Epoch: 75, Loss: 0.5678, Train: 68.99%, Valid: 69.05% Test: 69.13%\n",
            "Epoch: 76, Loss: 0.5623, Train: 68.64%, Valid: 68.50% Test: 69.52%\n",
            "Epoch: 77, Loss: 0.5567, Train: 69.59%, Valid: 69.33% Test: 69.41%\n",
            "Epoch: 78, Loss: 0.5739, Train: 69.38%, Valid: 69.40% Test: 69.87%\n",
            "Epoch: 79, Loss: 0.5657, Train: 70.02%, Valid: 69.44% Test: 69.81%\n",
            "Epoch: 80, Loss: 0.5685, Train: 70.21%, Valid: 69.28% Test: 69.78%\n",
            "Epoch: 81, Loss: 0.5530, Train: 69.96%, Valid: 69.44% Test: 70.47%\n",
            "Epoch: 82, Loss: 0.5779, Train: 70.16%, Valid: 69.16% Test: 70.41%\n",
            "Epoch: 83, Loss: 0.5682, Train: 69.84%, Valid: 70.15% Test: 70.24%\n",
            "Epoch: 84, Loss: 0.5785, Train: 69.48%, Valid: 69.84% Test: 70.10%\n",
            "Epoch: 85, Loss: 0.5478, Train: 69.44%, Valid: 69.40% Test: 69.90%\n",
            "Epoch: 86, Loss: 0.5672, Train: 69.76%, Valid: 70.11% Test: 70.19%\n",
            "Epoch: 87, Loss: 0.5728, Train: 69.73%, Valid: 70.11% Test: 69.88%\n",
            "Epoch: 88, Loss: 0.5243, Train: 69.46%, Valid: 69.74% Test: 70.13%\n",
            "Epoch: 89, Loss: 0.5539, Train: 70.49%, Valid: 69.65% Test: 69.99%\n",
            "Epoch: 90, Loss: 0.5568, Train: 70.59%, Valid: 69.42% Test: 70.53%\n",
            "Epoch: 91, Loss: 0.5416, Train: 71.12%, Valid: 69.72% Test: 70.87%\n",
            "Epoch: 92, Loss: 0.5442, Train: 70.92%, Valid: 69.99% Test: 70.35%\n",
            "Epoch: 93, Loss: 0.5447, Train: 70.95%, Valid: 70.09% Test: 70.63%\n",
            "Epoch: 94, Loss: 0.5424, Train: 70.82%, Valid: 69.56% Test: 70.76%\n",
            "Epoch: 95, Loss: 0.5556, Train: 70.35%, Valid: 69.68% Test: 70.69%\n",
            "Epoch: 96, Loss: 0.5229, Train: 70.81%, Valid: 69.95% Test: 70.52%\n",
            "Epoch: 97, Loss: 0.5420, Train: 70.46%, Valid: 70.09% Test: 70.84%\n",
            "Epoch: 98, Loss: 0.5382, Train: 71.07%, Valid: 69.90% Test: 71.04%\n",
            "Epoch: 99, Loss: 0.5404, Train: 71.01%, Valid: 70.39% Test: 71.24%\n",
            "Epoch: 100, Loss: 0.5358, Train: 71.17%, Valid: 70.25% Test: 71.09%\n",
            "Epoch: 101, Loss: 0.5525, Train: 71.71%, Valid: 70.84% Test: 71.79%\n",
            "Epoch: 102, Loss: 0.5353, Train: 71.78%, Valid: 70.34% Test: 71.69%\n",
            "Epoch: 103, Loss: 0.5372, Train: 72.28%, Valid: 70.13% Test: 71.86%\n",
            "Epoch: 104, Loss: 0.5611, Train: 71.85%, Valid: 70.00% Test: 71.77%\n",
            "Epoch: 105, Loss: 0.5369, Train: 71.76%, Valid: 70.46% Test: 71.30%\n",
            "Epoch: 106, Loss: 0.5475, Train: 71.31%, Valid: 70.04% Test: 71.42%\n",
            "Epoch: 107, Loss: 0.5187, Train: 71.50%, Valid: 69.72% Test: 70.91%\n",
            "Epoch: 108, Loss: 0.5310, Train: 71.41%, Valid: 70.03% Test: 71.02%\n",
            "Epoch: 109, Loss: 0.5220, Train: 71.44%, Valid: 70.26% Test: 71.24%\n",
            "Epoch: 110, Loss: 0.5722, Train: 71.33%, Valid: 70.15% Test: 71.88%\n",
            "Epoch: 111, Loss: 0.5193, Train: 72.13%, Valid: 70.35% Test: 71.94%\n",
            "Epoch: 112, Loss: 0.5375, Train: 72.17%, Valid: 70.61% Test: 72.38%\n",
            "Epoch: 113, Loss: 0.5064, Train: 72.01%, Valid: 71.34% Test: 72.28%\n",
            "Epoch: 114, Loss: 0.5106, Train: 72.76%, Valid: 70.32% Test: 72.77%\n",
            "Epoch: 115, Loss: 0.5259, Train: 72.56%, Valid: 70.41% Test: 72.64%\n",
            "Epoch: 116, Loss: 0.5222, Train: 72.73%, Valid: 70.93% Test: 72.34%\n",
            "Epoch: 117, Loss: 0.5261, Train: 72.50%, Valid: 70.61% Test: 72.13%\n",
            "Epoch: 118, Loss: 0.5061, Train: 71.89%, Valid: 70.41% Test: 71.69%\n",
            "Epoch: 119, Loss: 0.5093, Train: 72.07%, Valid: 70.70% Test: 71.92%\n",
            "Epoch: 120, Loss: 0.5096, Train: 72.21%, Valid: 70.30% Test: 71.62%\n",
            "Epoch: 121, Loss: 0.5203, Train: 72.31%, Valid: 70.66% Test: 71.83%\n",
            "Epoch: 122, Loss: 0.5236, Train: 72.78%, Valid: 70.59% Test: 71.94%\n",
            "Epoch: 123, Loss: 0.5123, Train: 72.52%, Valid: 70.89% Test: 71.89%\n",
            "Epoch: 124, Loss: 0.5167, Train: 72.61%, Valid: 70.70% Test: 71.83%\n",
            "Epoch: 125, Loss: 0.5122, Train: 72.48%, Valid: 71.09% Test: 72.28%\n",
            "Epoch: 126, Loss: 0.5427, Train: 72.63%, Valid: 71.02% Test: 72.01%\n",
            "Epoch: 127, Loss: 0.5275, Train: 72.33%, Valid: 70.47% Test: 71.76%\n",
            "Epoch: 128, Loss: 0.5150, Train: 72.24%, Valid: 70.83% Test: 71.69%\n",
            "Epoch: 129, Loss: 0.5047, Train: 71.76%, Valid: 70.78% Test: 71.29%\n",
            "Epoch: 130, Loss: 0.4960, Train: 72.32%, Valid: 71.26% Test: 72.01%\n",
            "Epoch: 131, Loss: 0.5003, Train: 72.01%, Valid: 71.09% Test: 71.96%\n",
            "Epoch: 132, Loss: 0.5049, Train: 72.54%, Valid: 71.03% Test: 72.11%\n",
            "Epoch: 133, Loss: 0.5099, Train: 73.43%, Valid: 71.17% Test: 72.38%\n",
            "Epoch: 134, Loss: 0.4986, Train: 73.07%, Valid: 70.72% Test: 72.61%\n",
            "Epoch: 135, Loss: 0.5149, Train: 73.27%, Valid: 71.26% Test: 72.94%\n",
            "Epoch: 136, Loss: 0.5241, Train: 72.95%, Valid: 70.73% Test: 72.78%\n",
            "Epoch: 137, Loss: 0.4981, Train: 73.45%, Valid: 70.79% Test: 72.40%\n",
            "Epoch: 138, Loss: 0.5156, Train: 72.62%, Valid: 71.39% Test: 72.49%\n",
            "Epoch: 139, Loss: 0.5026, Train: 72.49%, Valid: 70.88% Test: 72.03%\n",
            "Epoch: 140, Loss: 0.5074, Train: 72.64%, Valid: 71.22% Test: 72.42%\n",
            "Epoch: 141, Loss: 0.5125, Train: 72.87%, Valid: 71.33% Test: 72.47%\n",
            "Epoch: 142, Loss: 0.5138, Train: 72.84%, Valid: 70.87% Test: 72.37%\n",
            "Epoch: 143, Loss: 0.5013, Train: 73.29%, Valid: 71.40% Test: 72.50%\n",
            "Epoch: 144, Loss: 0.5117, Train: 73.16%, Valid: 71.07% Test: 72.71%\n",
            "Epoch: 145, Loss: 0.4964, Train: 73.05%, Valid: 71.25% Test: 72.78%\n",
            "Epoch: 146, Loss: 0.5126, Train: 72.77%, Valid: 71.58% Test: 72.69%\n",
            "Epoch: 147, Loss: 0.5111, Train: 73.07%, Valid: 71.52% Test: 72.27%\n",
            "Epoch: 148, Loss: 0.4921, Train: 72.86%, Valid: 71.63% Test: 72.31%\n",
            "Epoch: 149, Loss: 0.5208, Train: 73.15%, Valid: 71.03% Test: 72.06%\n",
            "Epoch: 150, Loss: 0.5066, Train: 73.28%, Valid: 71.52% Test: 72.31%\n",
            "Epoch: 151, Loss: 0.5042, Train: 72.77%, Valid: 71.60% Test: 72.65%\n",
            "Epoch: 152, Loss: 0.5013, Train: 72.78%, Valid: 71.04% Test: 72.86%\n",
            "Epoch: 153, Loss: 0.4999, Train: 73.22%, Valid: 71.60% Test: 72.75%\n",
            "Epoch: 154, Loss: 0.5078, Train: 72.78%, Valid: 71.36% Test: 73.03%\n",
            "Epoch: 155, Loss: 0.4931, Train: 73.30%, Valid: 71.00% Test: 72.88%\n",
            "Epoch: 156, Loss: 0.5062, Train: 72.87%, Valid: 71.35% Test: 72.33%\n",
            "Epoch: 157, Loss: 0.5050, Train: 72.75%, Valid: 71.62% Test: 72.36%\n",
            "Epoch: 158, Loss: 0.4979, Train: 72.63%, Valid: 70.91% Test: 72.17%\n",
            "Epoch: 159, Loss: 0.4922, Train: 72.73%, Valid: 71.23% Test: 72.12%\n",
            "Epoch: 160, Loss: 0.4980, Train: 72.85%, Valid: 71.65% Test: 72.34%\n",
            "Epoch: 161, Loss: 0.5011, Train: 73.07%, Valid: 71.71% Test: 72.46%\n",
            "Epoch: 162, Loss: 0.4943, Train: 72.93%, Valid: 71.29% Test: 72.65%\n",
            "Epoch: 163, Loss: 0.4885, Train: 73.45%, Valid: 71.46% Test: 72.64%\n",
            "Epoch: 164, Loss: 0.4936, Train: 73.48%, Valid: 71.72% Test: 73.17%\n",
            "Epoch: 165, Loss: 0.5023, Train: 73.50%, Valid: 71.78% Test: 72.80%\n",
            "Epoch: 166, Loss: 0.4865, Train: 73.27%, Valid: 71.68% Test: 73.04%\n",
            "Epoch: 167, Loss: 0.5006, Train: 73.30%, Valid: 71.49% Test: 72.85%\n",
            "Epoch: 168, Loss: 0.4964, Train: 72.84%, Valid: 71.73% Test: 72.53%\n",
            "Epoch: 169, Loss: 0.5105, Train: 72.50%, Valid: 71.88% Test: 72.54%\n",
            "Epoch: 170, Loss: 0.4946, Train: 72.79%, Valid: 72.40% Test: 72.38%\n",
            "Epoch: 171, Loss: 0.4963, Train: 72.68%, Valid: 72.23% Test: 72.69%\n",
            "Epoch: 172, Loss: 0.4934, Train: 73.26%, Valid: 71.58% Test: 72.94%\n",
            "Epoch: 173, Loss: 0.4907, Train: 73.55%, Valid: 71.78% Test: 72.92%\n",
            "Epoch: 174, Loss: 0.4999, Train: 73.30%, Valid: 72.22% Test: 72.98%\n",
            "Epoch: 175, Loss: 0.4957, Train: 73.48%, Valid: 72.17% Test: 73.03%\n",
            "Epoch: 176, Loss: 0.4918, Train: 73.15%, Valid: 72.25% Test: 73.03%\n",
            "Epoch: 177, Loss: 0.5026, Train: 73.25%, Valid: 71.82% Test: 72.83%\n",
            "Epoch: 178, Loss: 0.4924, Train: 73.26%, Valid: 71.42% Test: 72.92%\n",
            "Epoch: 179, Loss: 0.4931, Train: 73.44%, Valid: 71.23% Test: 72.82%\n",
            "Epoch: 180, Loss: 0.4886, Train: 73.38%, Valid: 71.54% Test: 72.88%\n",
            "Epoch: 181, Loss: 0.4860, Train: 73.48%, Valid: 71.49% Test: 72.74%\n",
            "Epoch: 182, Loss: 0.4854, Train: 73.53%, Valid: 71.32% Test: 72.69%\n",
            "Epoch: 183, Loss: 0.4830, Train: 73.45%, Valid: 71.67% Test: 72.62%\n",
            "Epoch: 184, Loss: 0.4939, Train: 73.25%, Valid: 71.64% Test: 73.03%\n",
            "Epoch: 185, Loss: 0.4946, Train: 73.75%, Valid: 72.16% Test: 72.83%\n",
            "Epoch: 186, Loss: 0.4852, Train: 73.63%, Valid: 72.42% Test: 72.71%\n",
            "Epoch: 187, Loss: 0.5000, Train: 72.90%, Valid: 71.85% Test: 72.86%\n",
            "Epoch: 188, Loss: 0.4931, Train: 73.10%, Valid: 71.73% Test: 72.59%\n",
            "Epoch: 189, Loss: 0.4866, Train: 73.41%, Valid: 71.92% Test: 72.88%\n",
            "Epoch: 190, Loss: 0.4831, Train: 73.37%, Valid: 71.93% Test: 73.21%\n",
            "Epoch: 191, Loss: 0.4926, Train: 73.77%, Valid: 71.53% Test: 72.84%\n",
            "Epoch: 192, Loss: 0.4879, Train: 73.74%, Valid: 71.48% Test: 73.11%\n",
            "Epoch: 193, Loss: 0.4943, Train: 73.56%, Valid: 72.15% Test: 73.04%\n",
            "Epoch: 194, Loss: 0.4873, Train: 73.78%, Valid: 71.96% Test: 72.94%\n",
            "Epoch: 195, Loss: 0.4868, Train: 73.69%, Valid: 72.14% Test: 73.28%\n",
            "Epoch: 196, Loss: 0.4857, Train: 73.25%, Valid: 71.70% Test: 73.49%\n",
            "Epoch: 197, Loss: 0.4922, Train: 73.83%, Valid: 71.50% Test: 73.28%\n",
            "Epoch: 198, Loss: 0.4901, Train: 73.72%, Valid: 71.79% Test: 72.62%\n",
            "Epoch: 199, Loss: 0.4783, Train: 74.01%, Valid: 72.12% Test: 73.45%\n",
            "Epoch: 200, Loss: 0.4917, Train: 73.69%, Valid: 71.95% Test: 73.45%\n",
            "Epoch: 201, Loss: 0.4896, Train: 73.68%, Valid: 72.02% Test: 73.38%\n",
            "Epoch: 202, Loss: 0.4854, Train: 74.03%, Valid: 71.94% Test: 73.37%\n",
            "Epoch: 203, Loss: 0.4819, Train: 73.98%, Valid: 71.56% Test: 73.32%\n",
            "Epoch: 204, Loss: 0.4858, Train: 74.11%, Valid: 71.65% Test: 73.28%\n",
            "Epoch: 205, Loss: 0.4866, Train: 74.04%, Valid: 71.86% Test: 73.54%\n",
            "Epoch: 206, Loss: 0.4852, Train: 73.79%, Valid: 71.79% Test: 73.09%\n",
            "Epoch: 207, Loss: 0.4760, Train: 73.85%, Valid: 71.73% Test: 73.05%\n",
            "Epoch: 208, Loss: 0.4866, Train: 73.76%, Valid: 71.73% Test: 73.11%\n",
            "Epoch: 209, Loss: 0.4880, Train: 73.73%, Valid: 72.22% Test: 73.07%\n",
            "Epoch: 210, Loss: 0.4864, Train: 73.38%, Valid: 72.09% Test: 73.43%\n",
            "Epoch: 211, Loss: 0.4827, Train: 74.07%, Valid: 72.00% Test: 73.38%\n",
            "Epoch: 212, Loss: 0.4982, Train: 73.83%, Valid: 71.97% Test: 73.42%\n",
            "Epoch: 213, Loss: 0.4925, Train: 74.02%, Valid: 72.48% Test: 73.23%\n",
            "Epoch: 214, Loss: 0.4851, Train: 73.87%, Valid: 72.33% Test: 73.46%\n",
            "Epoch: 215, Loss: 0.4871, Train: 73.79%, Valid: 72.47% Test: 73.34%\n",
            "Epoch: 216, Loss: 0.4855, Train: 73.40%, Valid: 72.25% Test: 73.22%\n",
            "Epoch: 217, Loss: 0.4884, Train: 73.44%, Valid: 72.48% Test: 73.23%\n",
            "Epoch: 218, Loss: 0.4854, Train: 73.87%, Valid: 72.41% Test: 73.31%\n",
            "Epoch: 219, Loss: 0.4809, Train: 73.67%, Valid: 72.30% Test: 73.41%\n",
            "Epoch: 220, Loss: 0.4754, Train: 73.82%, Valid: 72.21% Test: 73.79%\n",
            "Epoch: 221, Loss: 0.4936, Train: 74.50%, Valid: 72.19% Test: 73.07%\n",
            "Epoch: 222, Loss: 0.4862, Train: 73.95%, Valid: 72.29% Test: 73.83%\n",
            "Epoch: 223, Loss: 0.4872, Train: 73.77%, Valid: 72.00% Test: 73.66%\n",
            "Epoch: 224, Loss: 0.4875, Train: 73.52%, Valid: 72.33% Test: 73.18%\n",
            "Epoch: 225, Loss: 0.4906, Train: 73.38%, Valid: 72.10% Test: 73.38%\n",
            "Epoch: 226, Loss: 0.4945, Train: 72.78%, Valid: 71.86% Test: 72.81%\n",
            "Epoch: 227, Loss: 0.4830, Train: 73.60%, Valid: 72.59% Test: 73.08%\n",
            "Epoch: 228, Loss: 0.4945, Train: 73.64%, Valid: 71.88% Test: 73.04%\n",
            "Epoch: 229, Loss: 0.4851, Train: 73.80%, Valid: 72.11% Test: 73.17%\n",
            "Epoch: 230, Loss: 0.4987, Train: 73.75%, Valid: 72.50% Test: 73.57%\n",
            "Epoch: 231, Loss: 0.4875, Train: 73.90%, Valid: 72.24% Test: 72.83%\n",
            "Epoch: 232, Loss: 0.4788, Train: 73.90%, Valid: 71.73% Test: 73.19%\n",
            "Epoch: 233, Loss: 0.4911, Train: 73.58%, Valid: 72.03% Test: 73.14%\n",
            "Epoch: 234, Loss: 0.4777, Train: 74.37%, Valid: 71.74% Test: 73.53%\n",
            "Epoch: 235, Loss: 0.4887, Train: 74.21%, Valid: 71.50% Test: 73.39%\n",
            "Epoch: 236, Loss: 0.4895, Train: 74.33%, Valid: 71.95% Test: 73.24%\n",
            "Epoch: 237, Loss: 0.4862, Train: 73.86%, Valid: 71.64% Test: 73.07%\n",
            "Epoch: 238, Loss: 0.4846, Train: 73.53%, Valid: 71.62% Test: 73.08%\n",
            "Epoch: 239, Loss: 0.4890, Train: 73.46%, Valid: 71.74% Test: 72.85%\n",
            "Epoch: 240, Loss: 0.4858, Train: 73.80%, Valid: 71.54% Test: 72.56%\n",
            "Epoch: 241, Loss: 0.4892, Train: 73.94%, Valid: 72.01% Test: 72.78%\n",
            "Epoch: 242, Loss: 0.4823, Train: 73.69%, Valid: 72.24% Test: 73.07%\n",
            "Epoch: 243, Loss: 0.4930, Train: 73.79%, Valid: 71.82% Test: 73.39%\n",
            "Epoch: 244, Loss: 0.4920, Train: 73.68%, Valid: 72.27% Test: 73.32%\n",
            "Epoch: 245, Loss: 0.4830, Train: 73.93%, Valid: 71.70% Test: 72.76%\n",
            "Epoch: 246, Loss: 0.4897, Train: 74.65%, Valid: 71.54% Test: 73.37%\n",
            "Epoch: 247, Loss: 0.4827, Train: 73.85%, Valid: 71.82% Test: 73.18%\n",
            "Epoch: 248, Loss: 0.4787, Train: 73.53%, Valid: 71.82% Test: 72.75%\n",
            "Epoch: 249, Loss: 0.4767, Train: 73.84%, Valid: 71.27% Test: 72.89%\n",
            "Epoch: 250, Loss: 0.4813, Train: 73.57%, Valid: 71.92% Test: 72.77%\n",
            "Epoch: 251, Loss: 0.4790, Train: 74.06%, Valid: 72.01% Test: 73.15%\n",
            "Epoch: 252, Loss: 0.4832, Train: 73.83%, Valid: 71.81% Test: 73.46%\n",
            "Epoch: 253, Loss: 0.4812, Train: 73.85%, Valid: 71.87% Test: 73.37%\n",
            "Epoch: 254, Loss: 0.4887, Train: 74.00%, Valid: 72.22% Test: 73.39%\n",
            "Epoch: 255, Loss: 0.4769, Train: 74.24%, Valid: 71.56% Test: 73.07%\n",
            "Epoch: 256, Loss: 0.4775, Train: 74.61%, Valid: 71.80% Test: 72.97%\n",
            "Epoch: 257, Loss: 0.4926, Train: 74.81%, Valid: 71.45% Test: 73.17%\n",
            "Epoch: 258, Loss: 0.4854, Train: 74.51%, Valid: 71.54% Test: 73.17%\n",
            "Epoch: 259, Loss: 0.4823, Train: 74.15%, Valid: 71.93% Test: 73.46%\n",
            "Epoch: 260, Loss: 0.4804, Train: 74.17%, Valid: 72.22% Test: 73.65%\n",
            "Epoch: 261, Loss: 0.4912, Train: 74.26%, Valid: 72.39% Test: 73.68%\n",
            "Epoch: 262, Loss: 0.4818, Train: 74.46%, Valid: 72.12% Test: 73.56%\n",
            "Epoch: 263, Loss: 0.4845, Train: 74.75%, Valid: 72.31% Test: 74.08%\n",
            "Epoch: 264, Loss: 0.4788, Train: 74.82%, Valid: 72.43% Test: 74.04%\n",
            "Epoch: 265, Loss: 0.4851, Train: 74.46%, Valid: 72.72% Test: 73.96%\n",
            "Epoch: 266, Loss: 0.4825, Train: 73.93%, Valid: 72.26% Test: 73.82%\n",
            "Epoch: 267, Loss: 0.4859, Train: 74.40%, Valid: 71.98% Test: 74.06%\n",
            "Epoch: 268, Loss: 0.4811, Train: 74.35%, Valid: 72.12% Test: 73.93%\n",
            "Epoch: 269, Loss: 0.4856, Train: 74.30%, Valid: 71.58% Test: 73.59%\n",
            "Epoch: 270, Loss: 0.4869, Train: 74.46%, Valid: 72.00% Test: 73.92%\n",
            "Epoch: 271, Loss: 0.4782, Train: 74.40%, Valid: 72.25% Test: 74.00%\n",
            "Epoch: 272, Loss: 0.4797, Train: 74.29%, Valid: 71.58% Test: 74.04%\n",
            "Epoch: 273, Loss: 0.4786, Train: 74.54%, Valid: 72.08% Test: 73.71%\n",
            "Epoch: 274, Loss: 0.4801, Train: 74.21%, Valid: 72.23% Test: 73.62%\n",
            "Epoch: 275, Loss: 0.4739, Train: 74.24%, Valid: 71.70% Test: 73.57%\n",
            "Epoch: 276, Loss: 0.4832, Train: 73.49%, Valid: 72.35% Test: 73.82%\n",
            "Epoch: 277, Loss: 0.4857, Train: 74.29%, Valid: 72.39% Test: 73.79%\n",
            "Epoch: 278, Loss: 0.4865, Train: 74.11%, Valid: 71.61% Test: 73.56%\n",
            "Epoch: 279, Loss: 0.4754, Train: 74.56%, Valid: 71.99% Test: 73.72%\n",
            "Epoch: 280, Loss: 0.4787, Train: 74.12%, Valid: 72.20% Test: 73.11%\n",
            "Epoch: 281, Loss: 0.4794, Train: 73.80%, Valid: 72.03% Test: 73.08%\n",
            "Epoch: 282, Loss: 0.4786, Train: 73.34%, Valid: 72.41% Test: 73.12%\n",
            "Epoch: 283, Loss: 0.4777, Train: 73.64%, Valid: 72.14% Test: 72.60%\n",
            "Epoch: 284, Loss: 0.4769, Train: 73.31%, Valid: 72.26% Test: 73.17%\n",
            "Epoch: 285, Loss: 0.4845, Train: 73.51%, Valid: 73.00% Test: 73.58%\n",
            "Epoch: 286, Loss: 0.4777, Train: 74.06%, Valid: 72.29% Test: 73.86%\n",
            "Epoch: 287, Loss: 0.4780, Train: 74.03%, Valid: 72.60% Test: 73.89%\n",
            "Epoch: 288, Loss: 0.4785, Train: 74.37%, Valid: 72.61% Test: 74.07%\n",
            "Epoch: 289, Loss: 0.4755, Train: 74.23%, Valid: 72.82% Test: 73.78%\n",
            "Epoch: 290, Loss: 0.4706, Train: 74.60%, Valid: 72.93% Test: 74.22%\n",
            "Epoch: 291, Loss: 0.4876, Train: 74.06%, Valid: 73.03% Test: 73.71%\n",
            "Epoch: 292, Loss: 0.4758, Train: 74.63%, Valid: 72.94% Test: 74.05%\n",
            "Epoch: 293, Loss: 0.4784, Train: 74.15%, Valid: 72.49% Test: 74.04%\n",
            "Epoch: 294, Loss: 0.4781, Train: 74.66%, Valid: 72.03% Test: 74.23%\n",
            "Epoch: 295, Loss: 0.4742, Train: 74.22%, Valid: 72.42% Test: 74.17%\n",
            "Epoch: 296, Loss: 0.4793, Train: 74.49%, Valid: 72.50% Test: 73.90%\n",
            "Epoch: 297, Loss: 0.4806, Train: 74.06%, Valid: 72.51% Test: 73.89%\n",
            "Epoch: 298, Loss: 0.4725, Train: 74.26%, Valid: 72.25% Test: 73.39%\n",
            "Epoch: 299, Loss: 0.4672, Train: 74.87%, Valid: 72.21% Test: 73.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgQKg2KvHR7o",
        "outputId": "1c0ecdb2-1884-4285-f70f-dcc4feb2bf9d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7423, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(), \"StyleMind_GCN.pth\")"
      ],
      "metadata": {
        "id": "Y3HYYbneImY_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.eval()\n",
        "test_acc = test(best_model, x, test_message_passing_edges.to(device)\n",
        "    , test_eval_outfits.to(device), test_labels.to(device))\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKP9dC22HIuM",
        "outputId": "cefc07f9-9439-4a2d-b565-6927890f66de"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7419, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.title(\"StyleMind\")\n",
        "plt.plot(losses[3:], label=\"training loss\" + \" - \" + \"StyleMind-GCN\")\n",
        "plt.plot(test_accs[3:], label=\"test accuracy\" + \" - \" + \"StyleMind-GCN\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "No16L9nr1swB",
        "outputId": "606a9271-783b-46bb-ef5a-856bc85e5c39"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model = StyleMindGCN(in_channels, hidden_channels\n",
        "                     ,out_channels, number_of_layers,\"MLP\", dropout).to(device)"
      ],
      "metadata": {
        "id": "TqbjdQXdI5Ww"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model.reset_parameters()\n",
        "\n",
        "losses = []\n",
        "test_accs = []\n",
        "\n",
        "best_test_acc = 0\n",
        "best_mlp_model = None\n",
        "\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "x = x.to(device)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    train_positives_eval_outfits, message_passing_edges = split_train_data(train_edge_index, 5)\n",
        "    train_negatives_eval_outfits = negative_train_edge_index[torch.randperm(negative_train_edge_index.shape[0])[:train_positives_eval_outfits.shape[0]]]\n",
        "    train_labels = torch.cat((torch.ones(train_positives_eval_outfits.shape[0]),\n",
        "                              torch.zeros(train_negatives_eval_outfits.shape[0])))\n",
        "    train_eval_outfits = torch.cat((train_positives_eval_outfits, train_negatives_eval_outfits), dim=0)\n",
        "    train_loss = train(mlp_model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device),\n",
        "                       loss_fn=loss_fn, optimizer=optimizer)\n",
        "    train_acc = test(mlp_model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device))\n",
        "\n",
        "\n",
        "    valid_message_passing_edges = torch.cat((concat_edge_indices(train_positives_eval_outfits)\n",
        "    , message_passing_edges), dim=-1)\n",
        "    valid_labels = torch.cat((torch.ones(valid_edge_index.shape[0]),\n",
        "                              torch.zeros(valid_negative_samples.shape[0])))\n",
        "    valid_eval_outfits = torch.cat((valid_edge_index, valid_negative_samples), dim=0)\n",
        "    valid_acc = test(mlp_model, x, valid_message_passing_edges.to(device)\n",
        "    , valid_eval_outfits.to(device), valid_labels.to(device))\n",
        "\n",
        "\n",
        "\n",
        "    test_message_passing_edges = torch.cat((concat_edge_indices(valid_edge_index)\n",
        "    , valid_message_passing_edges), dim=-1)\n",
        "    test_labels = torch.cat((torch.ones(test_edge_index.shape[0]),\n",
        "                              torch.zeros(test_negative_samples.shape[0])))\n",
        "    test_eval_outfits = torch.cat((test_edge_index, test_negative_samples), dim=0)\n",
        "    test_acc = test(mlp_model, x, test_message_passing_edges.to(device)\n",
        "    , test_eval_outfits.to(device), test_labels.to(device))\n",
        "\n",
        "    if(test_acc > best_test_acc):\n",
        "        best_test_acc = test_acc\n",
        "        best_mlp_model = copy.deepcopy(mlp_model)\n",
        "\n",
        "    losses.append(train_loss)\n",
        "    test_accs.append(test_acc)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {train_loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "wwSwyILyJDNQ",
        "outputId": "8e0b15f1-5590-4654-ef06-8e65db5f4337"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Linear.forward() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-4222ef10bed9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                               torch.zeros(train_negatives_eval_outfits.shape[0])))\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_eval_outfits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_positives_eval_outfits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_negatives_eval_outfits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     train_loss = train(mlp_model, x, message_passing_edges.to(device)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_outfits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                        loss_fn=loss_fn, optimizer=optimizer)\n",
            "\u001b[0;32m<ipython-input-19-e0f101cbd21b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, edge_index, outfits, labels, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0membds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-047233aefab8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, outfits)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Linear.forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    }
  ]
}