{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5ooj+iHYLfCNcH6pj1uhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alijgh003/StyleMind-GNN/blob/main/StyleMind_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ilOURpAYkjuf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ejZ_JBMqRQu",
        "outputId": "cd9262fc-2240-4c53-d803-eaa890ad063c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "torch_version = str(torch.__version__)\n",
        "# scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "# sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "# !pip install torch-scatter -f $scatter_src\n",
        "# !pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "j54KJfvCqx5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85180b3e-18b1-49c9-a50f-941b7eb48552"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = \"/content/drive/MyDrive/StyleMind-GNN\""
      ],
      "metadata": {
        "id": "zv1V_GrYq5i3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path"
      ],
      "metadata": {
        "id": "rgDdePkytykH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "items = pd.read_csv(path.join(datasets_path, \"new_items.csv\"))\n",
        "items.set_index(\"ID\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "twTtZX4HAX9j",
        "outputId": "f6201903-d434-481a-de80-48832bb54af9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Unnamed: 0   index                                       url_name  \\\n",
              "ID                                                                             \n",
              "211990161           0       0                   neck print chiffon plus size   \n",
              "183179503           1       1  christian pellizzari floral jacquard trousers   \n",
              "152771755           2       2            natures jewelry stainless steel not   \n",
              "190445143           3       3                balmain women high waist flared   \n",
              "211444470           4       4                  acler orson high waist belted   \n",
              "...               ...     ...                                            ...   \n",
              "108411005      251003  251003          arnica embellished hand painted skirt   \n",
              "212452593      251004  251004                 thom browne little boy knitted   \n",
              "208651882      251005  251005              colorful vortex print shawl scarf   \n",
              "209210426      251006  251006               floral embroidered mesh see thru   \n",
              "181567392      251007  251007        shein sheinside extreme destroyed denim   \n",
              "\n",
              "                                                 description  \\\n",
              "ID                                                             \n",
              "211990161                                                NaN   \n",
              "183179503  Gold and black silk blend floral jacquard trou...   \n",
              "152771755                                                NaN   \n",
              "190445143  Decorative gold colored buttons with lion deta...   \n",
              "211444470  This Acler Orson High Waist Belted Short featu...   \n",
              "...                                                      ...   \n",
              "108411005  Shop Stella Jean in our expertly curated in-se...   \n",
              "212452593                                                NaN   \n",
              "208651882                                                NaN   \n",
              "209210426                                                NaN   \n",
              "181567392                                                NaN   \n",
              "\n",
              "                                                  catgeories  \\\n",
              "ID                                                             \n",
              "211990161                                                NaN   \n",
              "183179503  [\"Women's Fashion\", 'Clothing', 'Pants', 'Chri...   \n",
              "152771755                                                NaN   \n",
              "190445143  [\"Women's Fashion\", 'Clothing', 'Pants', 'Balm...   \n",
              "211444470  [\"Women's Fashion\", 'Clothing', 'Shorts', 'Mar...   \n",
              "...                                                      ...   \n",
              "108411005  [\"Women's Fashion\", 'Clothing', 'Skirts', 'Kne...   \n",
              "212452593                                                NaN   \n",
              "208651882                                                NaN   \n",
              "209210426                                                NaN   \n",
              "181567392                                                NaN   \n",
              "\n",
              "                                                   title  \\\n",
              "ID                                                         \n",
              "211990161                                            NaN   \n",
              "183179503  Christian Pellizzari floral jacquard trousers   \n",
              "152771755                                            NaN   \n",
              "190445143     Balmain Women High Waist Flared Knit Pants   \n",
              "211444470            Acler Orson High Waist Belted Short   \n",
              "...                                                  ...   \n",
              "108411005          Arnica Embellished Hand Painted Skirt   \n",
              "212452593                                            NaN   \n",
              "208651882                                            NaN   \n",
              "209210426                                            NaN   \n",
              "181567392                                            NaN   \n",
              "\n",
              "                                                     related  category_id  \\\n",
              "ID                                                                          \n",
              "211990161                                                NaN            3   \n",
              "183179503  ['Floral pants', 'Grey pants', 'Print pants', ...            2   \n",
              "152771755                                                NaN           11   \n",
              "190445143  ['Balmain', 'Flared pants', 'High-waisted pant...            2   \n",
              "211444470  ['Short shorts', 'High-waisted shorts', 'High ...            2   \n",
              "...                                                      ...          ...   \n",
              "108411005                 ['Red skirt', 'Embellished skirt']            2   \n",
              "212452593                                                NaN            4   \n",
              "208651882                                                NaN            8   \n",
              "209210426                                                NaN            3   \n",
              "181567392                                                NaN            4   \n",
              "\n",
              "          semantic_category  name_len  \n",
              "ID                                     \n",
              "211990161              tops        28  \n",
              "183179503           bottoms        45  \n",
              "152771755         jewellery        35  \n",
              "190445143           bottoms        31  \n",
              "211444470           bottoms        29  \n",
              "...                     ...       ...  \n",
              "108411005           bottoms        37  \n",
              "212452593         outerwear        30  \n",
              "208651882           scarves        33  \n",
              "209210426              tops        32  \n",
              "181567392         outerwear        39  \n",
              "\n",
              "[251008 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fb9ba4c-e64d-410b-8c88-d488032c4011\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>url_name</th>\n",
              "      <th>description</th>\n",
              "      <th>catgeories</th>\n",
              "      <th>title</th>\n",
              "      <th>related</th>\n",
              "      <th>category_id</th>\n",
              "      <th>semantic_category</th>\n",
              "      <th>name_len</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>211990161</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>neck print chiffon plus size</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>tops</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183179503</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>christian pellizzari floral jacquard trousers</td>\n",
              "      <td>Gold and black silk blend floral jacquard trou...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Pants', 'Chri...</td>\n",
              "      <td>Christian Pellizzari floral jacquard trousers</td>\n",
              "      <td>['Floral pants', 'Grey pants', 'Print pants', ...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152771755</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>natures jewelry stainless steel not</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>jewellery</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190445143</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>balmain women high waist flared</td>\n",
              "      <td>Decorative gold colored buttons with lion deta...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Pants', 'Balm...</td>\n",
              "      <td>Balmain Women High Waist Flared Knit Pants</td>\n",
              "      <td>['Balmain', 'Flared pants', 'High-waisted pant...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211444470</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>acler orson high waist belted</td>\n",
              "      <td>This Acler Orson High Waist Belted Short featu...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Shorts', 'Mar...</td>\n",
              "      <td>Acler Orson High Waist Belted Short</td>\n",
              "      <td>['Short shorts', 'High-waisted shorts', 'High ...</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108411005</th>\n",
              "      <td>251003</td>\n",
              "      <td>251003</td>\n",
              "      <td>arnica embellished hand painted skirt</td>\n",
              "      <td>Shop Stella Jean in our expertly curated in-se...</td>\n",
              "      <td>[\"Women's Fashion\", 'Clothing', 'Skirts', 'Kne...</td>\n",
              "      <td>Arnica Embellished Hand Painted Skirt</td>\n",
              "      <td>['Red skirt', 'Embellished skirt']</td>\n",
              "      <td>2</td>\n",
              "      <td>bottoms</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212452593</th>\n",
              "      <td>251004</td>\n",
              "      <td>251004</td>\n",
              "      <td>thom browne little boy knitted</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>outerwear</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208651882</th>\n",
              "      <td>251005</td>\n",
              "      <td>251005</td>\n",
              "      <td>colorful vortex print shawl scarf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>scarves</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209210426</th>\n",
              "      <td>251006</td>\n",
              "      <td>251006</td>\n",
              "      <td>floral embroidered mesh see thru</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>tops</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181567392</th>\n",
              "      <td>251007</td>\n",
              "      <td>251007</td>\n",
              "      <td>shein sheinside extreme destroyed denim</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>outerwear</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>251008 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fb9ba4c-e64d-410b-8c88-d488032c4011')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9fb9ba4c-e64d-410b-8c88-d488032c4011 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9fb9ba4c-e64d-410b-8c88-d488032c4011');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f63b3479-037c-403e-a512-6df4719648ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f63b3479-037c-403e-a512-6df4719648ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f63b3479-037c-403e-a512-6df4719648ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.load(path.join(datasets_path, \"items_img2vec.pth\"))\n",
        "x, x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S81FJj9utstn",
        "outputId": "66a5638d-b513-4cc6-f184-50b07da82131"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.2134, 0.4643, 0.3334,  ..., 0.3668, 0.0379, 0.0898],\n",
              "         [0.5982, 0.7184, 0.2694,  ..., 0.2122, 0.6592, 0.0720],\n",
              "         [0.3819, 4.5064, 0.2355,  ..., 0.2252, 0.7095, 0.4182],\n",
              "         ...,\n",
              "         [0.0192, 0.2306, 0.6947,  ..., 0.2099, 0.2533, 0.1645],\n",
              "         [0.4130, 0.2520, 0.2924,  ..., 0.5590, 0.3986, 0.0838],\n",
              "         [0.0576, 1.2712, 1.8085,  ..., 0.6934, 0.9322, 0.0889]]),\n",
              " torch.Size([251008, 2048]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.dtype ,x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQG4pouWuLVH",
        "outputId": "97330e4d-9ccd-447c-db6a-7aa5eaa582e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.Size([251008, 2048]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_indices_path = path.join(datasets_path, \"dataset\", \"polyvore_outfits\", \"nondisjoint\")"
      ],
      "metadata": {
        "id": "9mvzirN8tyMK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index = torch.load(path.join(edge_indices_path, \"train_edge_index.pth\"))\n",
        "test_edge_index = torch.load(path.join(edge_indices_path, \"test_edge_index.pth\"))\n",
        "valid_edge_index = torch.load(path.join(edge_indices_path, \"valid_edge_index.pth\"))"
      ],
      "metadata": {
        "id": "QpssZBHZuUue"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index.shape, valid_edge_index.shape, test_edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFN1Dp9um3w",
        "outputId": "e72310da-a502-4f78-8771-40f3c11c17dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([53306, 2, 171]),\n",
              " torch.Size([5000, 2, 91]),\n",
              " torch.Size([10000, 2, 136]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_node_embedding_with_mask_meaningless_edge_index_elements(x, edge_index):\n",
        "  mask = torch.where(edge_index==-1, torch.tensor(0), torch.tensor(1)).unsqueeze(-1)\n",
        "  r = x[edge_index] * mask\n",
        "  return r"
      ],
      "metadata": {
        "id": "n8pAOUMsqTxH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_edge_indices(edge_index_tensor_by_groups):\n",
        "  result = torch.cat([edge_index_tensor_by_groups[i] for i in range(edge_index_tensor_by_groups.shape[0])],dim=-1)\n",
        "  result = result[:,torch.where(result[0]>-1)[0]]\n",
        "  return result"
      ],
      "metadata": {
        "id": "vMyux7JYvApm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_edge_indices(train_edge_index[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCreRAxilaHj",
        "outputId": "0babda11-d481-4737-8bae-f970a242d0fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[204439, 154376, 154376, 154376, 154376, 154376, 154376, 154376, 191980,\n",
              "         191980, 191980, 192270, 192270, 192270, 192270,   6713,   6713,  78057,\n",
              "          78057,  78057,  78057,  78057, 100757, 100757, 100757, 100757, 100757,\n",
              "         100757],\n",
              "        [249298, 249298, 204439, 191980, 192270,   6713,  78057, 100757, 249298,\n",
              "         204439,   6713, 249298, 204439, 191980,   6713, 249298, 204439, 249298,\n",
              "         204439, 191980, 192270,   6713, 249298, 204439, 191980, 192270,   6713,\n",
              "          78057]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKBOTWU-09nR",
        "outputId": "f546b869-22fb-40a4-ea17-7da6c1298b89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([251008, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "n78uETa0vUTG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleMindGCN(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels ,number_of_layers, dropout=0.5):\n",
        "    super(StyleMindGCN,self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList([GCNConv(in_channels if i==0 else hidden_channels,hidden_channels if i<number_of_layers-1 else out_channels) for i in range(number_of_layers)])\n",
        "    self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_channels) for i in range(number_of_layers-1)])\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "        conv.reset_parameters()\n",
        "    for bn in self.bns:\n",
        "        bn.reset_parameters()\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, outfits):\n",
        "    # x is the tensor of node_features (i.e. ResNet representation of each image of each garment)\n",
        "    # edge_index is the (2, |E|) to represent edges between garments\n",
        "    # outfits are 3 dim tensors with shape of (|O|, 2 , |Maximum combination of garments in the outfits in dataset|)\n",
        "    edge_index = to_undirected(edge_index)\n",
        "\n",
        "    for conv, bn in zip(self.convs[:-1], self.bns):\n",
        "      x = conv(x, edge_index)\n",
        "      x = bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, self.dropout)\n",
        "\n",
        "    x = self.convs[-1](x,edge_index)\n",
        "\n",
        "    o = get_node_embedding_with_mask_meaningless_edge_index_elements(x,outfits)\n",
        "    # the output will be in the shape of (|O|, 2, |Maximum combination of garments in the outfits in dataset|)\n",
        "    o = (o[:,0,:] * o[:,1,:]).sum(dim=(-1,-2)) / (outfits[:,:1,:]>=0).float().sum(dim=(-1,-2))\n",
        "    # the output will be in the shape of |O|\n",
        "    o = F.sigmoid(o)\n",
        "    return x, o\n",
        "\n"
      ],
      "metadata": {
        "id": "hiDfVi00vJdO"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_UIhe-3K5yUS",
        "outputId": "a6c0d8d8-ce0c-4bbf-cfee-aed1d020905c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, x, edge_index, outfits, labels, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  embds,o = model(x, edge_index, outfits)\n",
        "\n",
        "  loss = loss_fn(o, labels)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "km5pFkE_5CLC"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, x, edge_index, outfits, labels):\n",
        "  model.eval()\n",
        "\n",
        "  embds,o = model(x, edge_index, outfits)\n",
        "  o_pred = torch.where(o>=0.5, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "  acc = (o_pred==labels).float().sum() / o_pred.shape[0]\n",
        "\n",
        "  return  acc"
      ],
      "metadata": {
        "id": "83SDJGCQ7Khn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = x.shape[-1]\n",
        "hidden_channels = 128\n",
        "out_channels = 128\n",
        "lr = 0.01\n",
        "number_of_layers = 3\n",
        "dropout = 0.5"
      ],
      "metadata": {
        "id": "_9GDYCrF8hNW"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StyleMindGCN(in_channels, hidden_channels\n",
        "                     ,out_channels, number_of_layers, dropout).to(device)"
      ],
      "metadata": {
        "id": "vI9dHMxn8lRi"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ZNMMLHNbff",
        "outputId": "aa44ec81-6a8f-47c8-adae-90d010d2503c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ngQnrxKxNi84"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_RQ-ogiLUWp",
        "outputId": "3ee101bb-4eb3-4350-bf08-d5145f9536ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory allocated: 1.13 MB\n",
            "Memory reserved: 2.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoKmKHQuJdmo",
        "outputId": "1246047a-37ba-4723-c23f-ad623aaa0e10"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([53306, 2, 171])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1000"
      ],
      "metadata": {
        "id": "eoKQAKK-9tLB"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "- [x] splitting train dataset to two part\n",
        "      - message_passing\n",
        "      - labels\n",
        "- [x] negative sampling  \n",
        "  naive negative sampling: We just have randomly selected\n",
        "    8 nodes to combine(2,nodes) to create some negative samples"
      ],
      "metadata": {
        "id": "HRYH1FTNEMz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def split_train_data(train_edge_index, diameter=8):\n",
        "  edge_nums =  train_edge_index.shape[0]\n",
        "  random_permutation = torch.randperm(edge_nums)\n",
        "  eval_outfits = train_edge_index[random_permutation[:edge_nums//diameter]]\n",
        "  message_passing_edges = concat_edge_indices(train_edge_index[random_permutation[edge_nums//diameter:]])\n",
        "  return eval_outfits, message_passing_edges\n",
        "\n",
        "a = split_train_data(train_edge_index)\n",
        "[value.shape for  value in a]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPILhMhvEyqD",
        "outputId": "b9fce250-d4ad-4db9-d0c4-9072d7178d08"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 362 ms, sys: 218 ms, total: 580 ms\n",
            "Wall time: 598 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([6663, 2, 171]), torch.Size([2, 600631])]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive negative sampling"
      ],
      "metadata": {
        "id": "gMQC8BPG6_J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "def create_random_negative_samples(num_set, num_samples, num_garment, padding_size):\n",
        "  combinations = [[list(itertools.combinations(torch.randperm(x.shape[0])[:num_garment].tolist(), 2)) for i in range(num_samples)] for j in range(num_set)]\n",
        "\n",
        "  tensor = torch.tensor(combinations).transpose(-1,-2)\n",
        "  tensor = torch.cat((tensor.flip(-2), tensor), dim=-1)\n",
        "  result = torch.full((num_set, num_samples, 2, padding_size), -1)\n",
        "  result[:,:,:,:tensor.shape[-1]] = tensor\n",
        "  return result\n",
        "\n",
        "negative_samples = create_random_negative_samples(10, 5000, 8, train_edge_index.shape[-1])\n",
        "negative_samples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q0oLDysG7ym",
        "outputId": "51ff9f68-8a7f-499f-f402-5abf7c6a99e4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 12s, sys: 512 ms, total: 4min 13s\n",
            "Wall time: 4min 14s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5000, 2, 171])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_negative_samples = create_random_negative_samples(1, valid_edge_index.shape[0],8, valid_edge_index.shape[-1])[0]\n",
        "test_negative_samples = create_random_negative_samples(1, test_edge_index.shape[0],8, test_edge_index.shape[-1])[0]\n",
        "valid_negative_samples.shape, test_negative_samples.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjoBWZt0v-15",
        "outputId": "1a299d9f-e924-434d-ee63-06c6e114565d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5000, 2, 91]), torch.Size([10000, 2, 136]))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_parameters()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "x = x.to(device)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    train_positives_eval_outfits, message_passing_edges = split_train_data(train_edge_index, 8)\n",
        "    train_negatives_eval_outfits = negative_samples[epoch % negative_samples.shape[0]]\n",
        "    train_labels = torch.cat((torch.ones(train_positives_eval_outfits.shape[0]),\n",
        "                              torch.zeros(train_negatives_eval_outfits.shape[0])))\n",
        "    train_eval_outfits = torch.cat((train_positives_eval_outfits, train_negatives_eval_outfits), dim=0)\n",
        "    train_loss = train(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device),\n",
        "                       loss_fn=loss_fn, optimizer=optimizer)\n",
        "    train_acc = test(model, x, message_passing_edges.to(device)\n",
        "    , train_eval_outfits.to(device), train_labels.to(device))\n",
        "\n",
        "\n",
        "    valid_message_passing_edges = torch.cat((concat_edge_indices(train_positives_eval_outfits)\n",
        "    , message_passing_edges), dim=-1)\n",
        "    valid_labels = torch.cat((torch.ones(valid_edge_index.shape[0]),\n",
        "                              torch.zeros(valid_negative_samples.shape[0])))\n",
        "    valid_eval_outfits = torch.cat((valid_edge_index, valid_negative_samples), dim=0)\n",
        "    valid_acc = test(model, x, valid_message_passing_edges.to(device)\n",
        "    , valid_eval_outfits.to(device), valid_labels.to(device))\n",
        "\n",
        "\n",
        "\n",
        "    test_message_passing_edges = torch.cat((concat_edge_indices(valid_edge_index)\n",
        "    , valid_message_passing_edges), dim=-1)\n",
        "    test_labels = torch.cat((torch.ones(test_edge_index.shape[0]),\n",
        "                              torch.zeros(test_negative_samples.shape[0])))\n",
        "    test_eval_outfits = torch.cat((test_edge_index, test_negative_samples), dim=0)\n",
        "    test_acc = test(model, x, test_message_passing_edges.to(device)\n",
        "    , test_eval_outfits.to(device), test_labels.to(device))\n",
        "\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {train_loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "XfIBGXzF9y51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "109feb72-fad4-4087-8481-0fd6d5bb3b6c"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 15.5558, Train: 57.13%, Valid: 50.00% Test: 49.99%\n",
            "Epoch: 01, Loss: 6.2417, Train: 57.02%, Valid: 49.93% Test: 49.92%\n",
            "Epoch: 02, Loss: 2.2776, Train: 57.07%, Valid: 49.98% Test: 49.95%\n",
            "Epoch: 03, Loss: 4.9023, Train: 57.10%, Valid: 49.97% Test: 49.97%\n",
            "Epoch: 04, Loss: 4.2414, Train: 57.06%, Valid: 49.93% Test: 49.95%\n",
            "Epoch: 05, Loss: 2.1029, Train: 57.04%, Valid: 49.97% Test: 49.94%\n",
            "Epoch: 06, Loss: 1.7390, Train: 57.10%, Valid: 49.99% Test: 49.98%\n",
            "Epoch: 07, Loss: 2.0399, Train: 57.10%, Valid: 49.98% Test: 50.00%\n",
            "Epoch: 08, Loss: 2.0631, Train: 57.12%, Valid: 49.98% Test: 49.99%\n",
            "Epoch: 09, Loss: 1.8429, Train: 57.04%, Valid: 49.93% Test: 49.92%\n",
            "Epoch: 10, Loss: 1.3993, Train: 57.04%, Valid: 49.92% Test: 49.88%\n",
            "Epoch: 11, Loss: 1.2240, Train: 56.98%, Valid: 49.81% Test: 49.76%\n",
            "Epoch: 12, Loss: 1.0704, Train: 56.88%, Valid: 49.83% Test: 49.83%\n",
            "Epoch: 13, Loss: 1.0720, Train: 56.97%, Valid: 49.80% Test: 49.79%\n",
            "Epoch: 14, Loss: 1.0925, Train: 56.81%, Valid: 49.62% Test: 49.79%\n",
            "Epoch: 15, Loss: 0.9955, Train: 56.76%, Valid: 49.56% Test: 49.67%\n",
            "Epoch: 16, Loss: 0.9425, Train: 56.34%, Valid: 49.36% Test: 49.39%\n",
            "Epoch: 17, Loss: 0.9156, Train: 56.13%, Valid: 49.22% Test: 49.22%\n",
            "Epoch: 18, Loss: 0.8003, Train: 56.04%, Valid: 49.11% Test: 49.31%\n",
            "Epoch: 19, Loss: 0.7914, Train: 55.98%, Valid: 48.94% Test: 49.11%\n",
            "Epoch: 20, Loss: 0.7957, Train: 56.01%, Valid: 49.07% Test: 49.20%\n",
            "Epoch: 21, Loss: 0.7691, Train: 56.05%, Valid: 49.16% Test: 49.22%\n",
            "Epoch: 22, Loss: 0.7645, Train: 56.19%, Valid: 49.06% Test: 49.21%\n",
            "Epoch: 23, Loss: 0.7331, Train: 56.03%, Valid: 49.17% Test: 49.22%\n",
            "Epoch: 24, Loss: 0.6897, Train: 55.98%, Valid: 49.15% Test: 49.04%\n",
            "Epoch: 25, Loss: 0.6851, Train: 55.85%, Valid: 48.83% Test: 48.96%\n",
            "Epoch: 26, Loss: 0.6683, Train: 56.01%, Valid: 48.93% Test: 48.77%\n",
            "Epoch: 27, Loss: 0.6369, Train: 55.83%, Valid: 48.77% Test: 48.84%\n",
            "Epoch: 28, Loss: 0.6428, Train: 55.82%, Valid: 48.92% Test: 49.09%\n",
            "Epoch: 29, Loss: 0.6425, Train: 56.09%, Valid: 49.16% Test: 49.07%\n",
            "Epoch: 30, Loss: 0.6131, Train: 55.92%, Valid: 49.38% Test: 49.21%\n",
            "Epoch: 31, Loss: 0.6376, Train: 56.12%, Valid: 49.25% Test: 49.37%\n",
            "Epoch: 32, Loss: 0.5964, Train: 56.52%, Valid: 49.35% Test: 49.33%\n",
            "Epoch: 33, Loss: 0.5708, Train: 56.79%, Valid: 49.58% Test: 49.53%\n",
            "Epoch: 34, Loss: 0.5876, Train: 56.89%, Valid: 50.00% Test: 49.61%\n",
            "Epoch: 35, Loss: 0.5767, Train: 57.24%, Valid: 50.27% Test: 50.19%\n",
            "Epoch: 36, Loss: 0.5835, Train: 57.83%, Valid: 50.97% Test: 50.53%\n",
            "Epoch: 37, Loss: 0.5609, Train: 58.58%, Valid: 52.30% Test: 51.28%\n",
            "Epoch: 38, Loss: 0.5654, Train: 58.80%, Valid: 52.80% Test: 51.73%\n",
            "Epoch: 39, Loss: 0.5661, Train: 59.26%, Valid: 53.08% Test: 52.31%\n",
            "Epoch: 40, Loss: 0.5397, Train: 59.48%, Valid: 53.17% Test: 52.67%\n",
            "Epoch: 41, Loss: 0.5574, Train: 59.57%, Valid: 54.09% Test: 52.96%\n",
            "Epoch: 42, Loss: 0.5372, Train: 60.10%, Valid: 54.35% Test: 53.61%\n",
            "Epoch: 43, Loss: 0.5248, Train: 60.95%, Valid: 55.00% Test: 54.24%\n",
            "Epoch: 44, Loss: 0.5361, Train: 61.22%, Valid: 55.86% Test: 55.02%\n",
            "Epoch: 45, Loss: 0.5302, Train: 62.45%, Valid: 57.21% Test: 55.84%\n",
            "Epoch: 46, Loss: 0.5262, Train: 62.72%, Valid: 58.37% Test: 57.02%\n",
            "Epoch: 47, Loss: 0.5336, Train: 64.31%, Valid: 59.23% Test: 58.15%\n",
            "Epoch: 48, Loss: 0.5169, Train: 64.31%, Valid: 60.31% Test: 59.45%\n",
            "Epoch: 49, Loss: 0.5239, Train: 65.69%, Valid: 61.41% Test: 60.13%\n",
            "Epoch: 50, Loss: 0.5113, Train: 66.33%, Valid: 62.32% Test: 60.97%\n",
            "Epoch: 51, Loss: 0.5176, Train: 66.65%, Valid: 63.13% Test: 62.10%\n",
            "Epoch: 52, Loss: 0.5101, Train: 67.65%, Valid: 63.78% Test: 62.77%\n",
            "Epoch: 53, Loss: 0.4994, Train: 67.91%, Valid: 64.60% Test: 63.44%\n",
            "Epoch: 54, Loss: 0.5090, Train: 67.60%, Valid: 65.68% Test: 64.52%\n",
            "Epoch: 55, Loss: 0.4994, Train: 69.25%, Valid: 66.60% Test: 65.88%\n",
            "Epoch: 56, Loss: 0.5061, Train: 70.27%, Valid: 67.27% Test: 66.49%\n",
            "Epoch: 57, Loss: 0.5047, Train: 70.90%, Valid: 68.49% Test: 67.72%\n",
            "Epoch: 58, Loss: 0.4953, Train: 71.41%, Valid: 69.78% Test: 68.14%\n",
            "Epoch: 59, Loss: 0.4973, Train: 72.00%, Valid: 70.50% Test: 69.54%\n",
            "Epoch: 60, Loss: 0.4954, Train: 72.93%, Valid: 71.09% Test: 69.58%\n",
            "Epoch: 61, Loss: 0.4922, Train: 72.81%, Valid: 71.09% Test: 70.10%\n",
            "Epoch: 62, Loss: 0.5084, Train: 73.43%, Valid: 71.55% Test: 70.09%\n",
            "Epoch: 63, Loss: 0.4861, Train: 73.31%, Valid: 71.59% Test: 70.18%\n",
            "Epoch: 64, Loss: 0.5042, Train: 72.99%, Valid: 71.73% Test: 70.52%\n",
            "Epoch: 65, Loss: 0.4881, Train: 73.45%, Valid: 71.82% Test: 70.84%\n",
            "Epoch: 66, Loss: 0.4922, Train: 73.58%, Valid: 72.53% Test: 71.41%\n",
            "Epoch: 67, Loss: 0.4884, Train: 73.98%, Valid: 73.26% Test: 71.20%\n",
            "Epoch: 68, Loss: 0.4864, Train: 74.34%, Valid: 73.20% Test: 71.56%\n",
            "Epoch: 69, Loss: 0.4872, Train: 73.43%, Valid: 73.43% Test: 71.90%\n",
            "Epoch: 70, Loss: 0.4830, Train: 73.75%, Valid: 73.28% Test: 72.26%\n",
            "Epoch: 71, Loss: 0.4913, Train: 73.77%, Valid: 73.39% Test: 71.87%\n",
            "Epoch: 72, Loss: 0.4756, Train: 74.91%, Valid: 73.22% Test: 71.93%\n",
            "Epoch: 73, Loss: 0.4844, Train: 74.28%, Valid: 73.19% Test: 71.40%\n",
            "Epoch: 74, Loss: 0.4826, Train: 74.29%, Valid: 73.15% Test: 70.99%\n",
            "Epoch: 75, Loss: 0.4729, Train: 75.16%, Valid: 72.66% Test: 71.72%\n",
            "Epoch: 76, Loss: 0.4781, Train: 74.31%, Valid: 72.90% Test: 71.41%\n",
            "Epoch: 77, Loss: 0.4828, Train: 74.06%, Valid: 72.94% Test: 71.23%\n",
            "Epoch: 78, Loss: 0.4766, Train: 74.95%, Valid: 73.59% Test: 71.45%\n",
            "Epoch: 79, Loss: 0.4785, Train: 74.67%, Valid: 73.47% Test: 71.37%\n",
            "Epoch: 80, Loss: 0.4702, Train: 75.55%, Valid: 74.00% Test: 72.19%\n",
            "Epoch: 81, Loss: 0.4774, Train: 74.75%, Valid: 74.10% Test: 72.05%\n",
            "Epoch: 82, Loss: 0.4784, Train: 75.42%, Valid: 74.42% Test: 72.40%\n",
            "Epoch: 83, Loss: 0.4714, Train: 75.15%, Valid: 74.03% Test: 72.53%\n",
            "Epoch: 84, Loss: 0.4787, Train: 74.54%, Valid: 74.60% Test: 72.38%\n",
            "Epoch: 85, Loss: 0.4680, Train: 75.11%, Valid: 74.47% Test: 72.21%\n",
            "Epoch: 86, Loss: 0.4715, Train: 75.40%, Valid: 74.42% Test: 72.53%\n",
            "Epoch: 87, Loss: 0.4742, Train: 75.44%, Valid: 74.77% Test: 72.53%\n",
            "Epoch: 88, Loss: 0.4664, Train: 75.02%, Valid: 74.30% Test: 72.25%\n",
            "Epoch: 89, Loss: 0.4688, Train: 75.17%, Valid: 74.59% Test: 72.97%\n",
            "Epoch: 90, Loss: 0.4677, Train: 76.10%, Valid: 73.99% Test: 72.87%\n",
            "Epoch: 91, Loss: 0.4724, Train: 75.98%, Valid: 74.81% Test: 73.31%\n",
            "Epoch: 92, Loss: 0.4613, Train: 76.58%, Valid: 74.80% Test: 72.93%\n",
            "Epoch: 93, Loss: 0.4633, Train: 76.28%, Valid: 74.65% Test: 73.40%\n",
            "Epoch: 94, Loss: 0.4663, Train: 75.99%, Valid: 75.42% Test: 73.36%\n",
            "Epoch: 95, Loss: 0.4616, Train: 76.60%, Valid: 75.02% Test: 73.42%\n",
            "Epoch: 96, Loss: 0.4660, Train: 76.31%, Valid: 74.47% Test: 72.96%\n",
            "Epoch: 97, Loss: 0.4659, Train: 76.04%, Valid: 74.54% Test: 72.88%\n",
            "Epoch: 98, Loss: 0.4661, Train: 76.01%, Valid: 74.28% Test: 72.52%\n",
            "Epoch: 99, Loss: 0.4622, Train: 76.52%, Valid: 74.73% Test: 72.27%\n",
            "Epoch: 100, Loss: 0.4621, Train: 76.34%, Valid: 74.38% Test: 72.36%\n",
            "Epoch: 101, Loss: 0.4625, Train: 76.04%, Valid: 74.52% Test: 72.48%\n",
            "Epoch: 102, Loss: 0.4544, Train: 76.55%, Valid: 74.42% Test: 72.49%\n",
            "Epoch: 103, Loss: 0.4628, Train: 75.88%, Valid: 74.50% Test: 72.58%\n",
            "Epoch: 104, Loss: 0.4589, Train: 75.98%, Valid: 74.98% Test: 72.56%\n",
            "Epoch: 105, Loss: 0.4585, Train: 77.83%, Valid: 75.01% Test: 72.74%\n",
            "Epoch: 106, Loss: 0.4551, Train: 76.67%, Valid: 74.32% Test: 72.17%\n",
            "Epoch: 107, Loss: 0.4589, Train: 76.62%, Valid: 74.49% Test: 72.11%\n",
            "Epoch: 108, Loss: 0.4561, Train: 76.11%, Valid: 74.48% Test: 72.15%\n",
            "Epoch: 109, Loss: 0.4534, Train: 76.59%, Valid: 74.29% Test: 72.60%\n",
            "Epoch: 110, Loss: 0.4567, Train: 76.52%, Valid: 74.76% Test: 72.86%\n",
            "Epoch: 111, Loss: 0.4651, Train: 76.03%, Valid: 74.57% Test: 73.33%\n",
            "Epoch: 112, Loss: 0.4523, Train: 76.68%, Valid: 74.54% Test: 73.18%\n",
            "Epoch: 113, Loss: 0.4518, Train: 76.70%, Valid: 75.15% Test: 73.11%\n",
            "Epoch: 114, Loss: 0.4623, Train: 76.90%, Valid: 75.08% Test: 73.10%\n",
            "Epoch: 115, Loss: 0.4547, Train: 76.71%, Valid: 74.89% Test: 72.83%\n",
            "Epoch: 116, Loss: 0.4567, Train: 76.58%, Valid: 74.65% Test: 72.63%\n",
            "Epoch: 117, Loss: 0.4548, Train: 76.64%, Valid: 74.05% Test: 72.42%\n",
            "Epoch: 118, Loss: 0.4616, Train: 76.30%, Valid: 74.44% Test: 72.26%\n",
            "Epoch: 119, Loss: 0.4526, Train: 76.52%, Valid: 74.70% Test: 72.50%\n",
            "Epoch: 120, Loss: 0.4562, Train: 76.28%, Valid: 74.16% Test: 72.57%\n",
            "Epoch: 121, Loss: 0.4488, Train: 76.70%, Valid: 75.06% Test: 72.73%\n",
            "Epoch: 122, Loss: 0.4510, Train: 76.61%, Valid: 74.95% Test: 73.11%\n",
            "Epoch: 123, Loss: 0.4519, Train: 76.47%, Valid: 75.01% Test: 73.25%\n",
            "Epoch: 124, Loss: 0.4484, Train: 76.76%, Valid: 74.86% Test: 73.68%\n",
            "Epoch: 125, Loss: 0.4452, Train: 76.69%, Valid: 74.85% Test: 72.96%\n",
            "Epoch: 126, Loss: 0.4576, Train: 76.06%, Valid: 74.87% Test: 73.10%\n",
            "Epoch: 127, Loss: 0.4561, Train: 76.34%, Valid: 74.86% Test: 73.24%\n",
            "Epoch: 128, Loss: 0.4461, Train: 76.39%, Valid: 74.89% Test: 73.11%\n",
            "Epoch: 129, Loss: 0.4493, Train: 75.61%, Valid: 74.47% Test: 73.21%\n",
            "Epoch: 130, Loss: 0.4540, Train: 75.81%, Valid: 74.85% Test: 73.62%\n",
            "Epoch: 131, Loss: 0.4462, Train: 76.58%, Valid: 74.45% Test: 73.60%\n",
            "Epoch: 132, Loss: 0.4439, Train: 77.17%, Valid: 75.27% Test: 73.33%\n",
            "Epoch: 133, Loss: 0.4506, Train: 76.64%, Valid: 75.25% Test: 73.11%\n",
            "Epoch: 134, Loss: 0.4463, Train: 76.70%, Valid: 74.98% Test: 73.61%\n",
            "Epoch: 135, Loss: 0.4454, Train: 76.53%, Valid: 75.34% Test: 73.47%\n",
            "Epoch: 136, Loss: 0.4522, Train: 76.78%, Valid: 75.20% Test: 73.82%\n",
            "Epoch: 137, Loss: 0.4481, Train: 76.82%, Valid: 75.82% Test: 73.52%\n",
            "Epoch: 138, Loss: 0.4506, Train: 76.94%, Valid: 75.43% Test: 73.75%\n",
            "Epoch: 139, Loss: 0.4467, Train: 77.17%, Valid: 75.76% Test: 73.72%\n",
            "Epoch: 140, Loss: 0.4440, Train: 77.32%, Valid: 75.49% Test: 73.98%\n",
            "Epoch: 141, Loss: 0.4410, Train: 77.36%, Valid: 75.86% Test: 74.82%\n",
            "Epoch: 142, Loss: 0.4413, Train: 77.66%, Valid: 75.85% Test: 74.51%\n",
            "Epoch: 143, Loss: 0.4421, Train: 77.65%, Valid: 76.48% Test: 75.04%\n",
            "Epoch: 144, Loss: 0.4443, Train: 76.53%, Valid: 75.96% Test: 74.61%\n",
            "Epoch: 145, Loss: 0.4388, Train: 78.01%, Valid: 76.19% Test: 75.12%\n",
            "Epoch: 146, Loss: 0.4495, Train: 78.01%, Valid: 76.19% Test: 74.57%\n",
            "Epoch: 147, Loss: 0.4486, Train: 77.73%, Valid: 75.88% Test: 73.83%\n",
            "Epoch: 148, Loss: 0.4439, Train: 77.93%, Valid: 75.30% Test: 74.00%\n",
            "Epoch: 149, Loss: 0.4371, Train: 77.82%, Valid: 76.04% Test: 74.08%\n",
            "Epoch: 150, Loss: 0.4427, Train: 77.52%, Valid: 76.39% Test: 74.74%\n",
            "Epoch: 151, Loss: 0.4368, Train: 77.80%, Valid: 76.06% Test: 74.61%\n",
            "Epoch: 152, Loss: 0.4390, Train: 77.60%, Valid: 76.16% Test: 74.71%\n",
            "Epoch: 153, Loss: 0.4397, Train: 77.52%, Valid: 75.93% Test: 74.39%\n",
            "Epoch: 154, Loss: 0.4357, Train: 77.81%, Valid: 76.31% Test: 74.78%\n",
            "Epoch: 155, Loss: 0.4398, Train: 78.02%, Valid: 76.32% Test: 74.44%\n",
            "Epoch: 156, Loss: 0.4445, Train: 77.54%, Valid: 76.37% Test: 74.25%\n",
            "Epoch: 157, Loss: 0.4420, Train: 77.44%, Valid: 76.29% Test: 74.13%\n",
            "Epoch: 158, Loss: 0.4422, Train: 77.81%, Valid: 76.39% Test: 74.28%\n",
            "Epoch: 159, Loss: 0.4402, Train: 77.60%, Valid: 76.10% Test: 73.89%\n",
            "Epoch: 160, Loss: 0.4440, Train: 77.00%, Valid: 76.50% Test: 74.43%\n",
            "Epoch: 161, Loss: 0.4335, Train: 77.82%, Valid: 76.28% Test: 74.28%\n",
            "Epoch: 162, Loss: 0.4366, Train: 77.84%, Valid: 76.27% Test: 74.43%\n",
            "Epoch: 163, Loss: 0.4401, Train: 77.31%, Valid: 76.35% Test: 74.31%\n",
            "Epoch: 164, Loss: 0.4394, Train: 77.59%, Valid: 75.78% Test: 74.35%\n",
            "Epoch: 165, Loss: 0.4393, Train: 77.97%, Valid: 75.82% Test: 74.32%\n",
            "Epoch: 166, Loss: 0.4360, Train: 77.86%, Valid: 76.01% Test: 73.72%\n",
            "Epoch: 167, Loss: 0.4396, Train: 77.84%, Valid: 75.83% Test: 73.83%\n",
            "Epoch: 168, Loss: 0.4396, Train: 77.65%, Valid: 75.87% Test: 73.86%\n",
            "Epoch: 169, Loss: 0.4415, Train: 78.00%, Valid: 76.09% Test: 73.78%\n",
            "Epoch: 170, Loss: 0.4435, Train: 77.75%, Valid: 75.48% Test: 74.06%\n",
            "Epoch: 171, Loss: 0.4415, Train: 77.56%, Valid: 75.88% Test: 73.72%\n",
            "Epoch: 172, Loss: 0.4307, Train: 77.69%, Valid: 75.45% Test: 73.57%\n",
            "Epoch: 173, Loss: 0.4379, Train: 77.97%, Valid: 75.45% Test: 73.01%\n",
            "Epoch: 174, Loss: 0.4359, Train: 77.48%, Valid: 75.10% Test: 72.56%\n",
            "Epoch: 175, Loss: 0.4364, Train: 77.24%, Valid: 74.92% Test: 72.89%\n",
            "Epoch: 176, Loss: 0.4368, Train: 77.59%, Valid: 75.22% Test: 72.56%\n",
            "Epoch: 177, Loss: 0.4262, Train: 78.48%, Valid: 74.91% Test: 72.68%\n",
            "Epoch: 178, Loss: 0.4361, Train: 77.35%, Valid: 75.82% Test: 73.06%\n",
            "Epoch: 179, Loss: 0.4356, Train: 77.98%, Valid: 76.18% Test: 73.94%\n",
            "Epoch: 180, Loss: 0.4361, Train: 77.53%, Valid: 76.16% Test: 74.17%\n",
            "Epoch: 181, Loss: 0.4345, Train: 77.75%, Valid: 75.71% Test: 74.49%\n",
            "Epoch: 182, Loss: 0.4348, Train: 78.09%, Valid: 76.54% Test: 74.35%\n",
            "Epoch: 183, Loss: 0.4378, Train: 78.46%, Valid: 75.97% Test: 74.32%\n",
            "Epoch: 184, Loss: 0.4341, Train: 78.35%, Valid: 75.74% Test: 74.40%\n",
            "Epoch: 185, Loss: 0.4291, Train: 78.39%, Valid: 75.68% Test: 74.46%\n",
            "Epoch: 186, Loss: 0.4392, Train: 77.18%, Valid: 76.15% Test: 74.40%\n",
            "Epoch: 187, Loss: 0.4306, Train: 77.45%, Valid: 75.78% Test: 73.98%\n",
            "Epoch: 188, Loss: 0.4366, Train: 77.34%, Valid: 75.83% Test: 74.14%\n",
            "Epoch: 189, Loss: 0.4353, Train: 77.30%, Valid: 75.64% Test: 74.10%\n",
            "Epoch: 190, Loss: 0.4379, Train: 78.07%, Valid: 75.41% Test: 73.98%\n",
            "Epoch: 191, Loss: 0.4293, Train: 77.97%, Valid: 75.64% Test: 74.61%\n",
            "Epoch: 192, Loss: 0.4299, Train: 78.04%, Valid: 76.05% Test: 74.26%\n",
            "Epoch: 193, Loss: 0.4336, Train: 78.26%, Valid: 75.90% Test: 74.01%\n",
            "Epoch: 194, Loss: 0.4271, Train: 78.43%, Valid: 75.97% Test: 73.86%\n",
            "Epoch: 195, Loss: 0.4317, Train: 77.31%, Valid: 76.07% Test: 74.00%\n",
            "Epoch: 196, Loss: 0.4325, Train: 77.64%, Valid: 75.96% Test: 74.04%\n",
            "Epoch: 197, Loss: 0.4300, Train: 78.20%, Valid: 75.80% Test: 74.09%\n",
            "Epoch: 198, Loss: 0.4312, Train: 77.87%, Valid: 76.26% Test: 74.72%\n",
            "Epoch: 199, Loss: 0.4299, Train: 77.86%, Valid: 76.18% Test: 75.11%\n",
            "Epoch: 200, Loss: 0.4287, Train: 78.33%, Valid: 75.82% Test: 74.65%\n",
            "Epoch: 201, Loss: 0.4366, Train: 77.39%, Valid: 76.67% Test: 74.92%\n",
            "Epoch: 202, Loss: 0.4290, Train: 77.66%, Valid: 76.22% Test: 74.71%\n",
            "Epoch: 203, Loss: 0.4307, Train: 78.02%, Valid: 76.33% Test: 74.48%\n",
            "Epoch: 204, Loss: 0.4312, Train: 77.87%, Valid: 76.11% Test: 74.35%\n",
            "Epoch: 205, Loss: 0.4324, Train: 78.43%, Valid: 75.75% Test: 74.33%\n",
            "Epoch: 206, Loss: 0.4300, Train: 78.25%, Valid: 76.13% Test: 74.57%\n",
            "Epoch: 207, Loss: 0.4280, Train: 78.76%, Valid: 76.21% Test: 74.17%\n",
            "Epoch: 208, Loss: 0.4269, Train: 78.51%, Valid: 76.53% Test: 74.64%\n",
            "Epoch: 209, Loss: 0.4323, Train: 78.16%, Valid: 76.18% Test: 74.47%\n",
            "Epoch: 210, Loss: 0.4336, Train: 78.80%, Valid: 76.66% Test: 74.22%\n",
            "Epoch: 211, Loss: 0.4265, Train: 78.19%, Valid: 75.76% Test: 73.57%\n",
            "Epoch: 212, Loss: 0.4250, Train: 78.58%, Valid: 75.77% Test: 73.79%\n",
            "Epoch: 213, Loss: 0.4335, Train: 78.13%, Valid: 75.90% Test: 73.69%\n",
            "Epoch: 214, Loss: 0.4275, Train: 78.41%, Valid: 76.04% Test: 73.81%\n",
            "Epoch: 215, Loss: 0.4269, Train: 78.51%, Valid: 76.02% Test: 73.75%\n",
            "Epoch: 216, Loss: 0.4297, Train: 78.58%, Valid: 75.67% Test: 73.33%\n",
            "Epoch: 217, Loss: 0.4229, Train: 78.76%, Valid: 75.85% Test: 73.09%\n",
            "Epoch: 218, Loss: 0.4255, Train: 78.75%, Valid: 75.75% Test: 73.10%\n",
            "Epoch: 219, Loss: 0.4283, Train: 78.66%, Valid: 75.64% Test: 73.64%\n",
            "Epoch: 220, Loss: 0.4311, Train: 78.81%, Valid: 76.20% Test: 73.97%\n",
            "Epoch: 221, Loss: 0.4292, Train: 78.62%, Valid: 75.67% Test: 73.35%\n",
            "Epoch: 222, Loss: 0.4224, Train: 78.59%, Valid: 75.74% Test: 73.46%\n",
            "Epoch: 223, Loss: 0.4261, Train: 78.23%, Valid: 76.24% Test: 73.71%\n",
            "Epoch: 224, Loss: 0.4269, Train: 78.98%, Valid: 75.71% Test: 73.32%\n",
            "Epoch: 225, Loss: 0.4197, Train: 78.68%, Valid: 75.43% Test: 73.24%\n",
            "Epoch: 226, Loss: 0.4247, Train: 78.56%, Valid: 75.72% Test: 73.25%\n",
            "Epoch: 227, Loss: 0.4249, Train: 78.62%, Valid: 75.26% Test: 73.34%\n",
            "Epoch: 228, Loss: 0.4254, Train: 78.44%, Valid: 75.92% Test: 73.25%\n",
            "Epoch: 229, Loss: 0.4234, Train: 78.60%, Valid: 76.05% Test: 73.36%\n",
            "Epoch: 230, Loss: 0.4260, Train: 78.32%, Valid: 75.55% Test: 73.78%\n",
            "Epoch: 231, Loss: 0.4261, Train: 78.38%, Valid: 75.71% Test: 73.66%\n",
            "Epoch: 232, Loss: 0.4264, Train: 78.80%, Valid: 75.57% Test: 73.55%\n",
            "Epoch: 233, Loss: 0.4200, Train: 78.33%, Valid: 75.99% Test: 73.17%\n",
            "Epoch: 234, Loss: 0.4273, Train: 78.17%, Valid: 75.69% Test: 73.01%\n",
            "Epoch: 235, Loss: 0.4193, Train: 78.92%, Valid: 75.68% Test: 73.04%\n",
            "Epoch: 236, Loss: 0.4241, Train: 78.26%, Valid: 75.02% Test: 73.29%\n",
            "Epoch: 237, Loss: 0.4236, Train: 77.99%, Valid: 75.32% Test: 73.39%\n",
            "Epoch: 238, Loss: 0.4219, Train: 78.42%, Valid: 75.77% Test: 73.47%\n",
            "Epoch: 239, Loss: 0.4207, Train: 78.07%, Valid: 75.61% Test: 73.43%\n",
            "Epoch: 240, Loss: 0.4190, Train: 78.08%, Valid: 75.30% Test: 73.36%\n",
            "Epoch: 241, Loss: 0.4186, Train: 77.79%, Valid: 75.13% Test: 73.49%\n",
            "Epoch: 242, Loss: 0.4199, Train: 78.10%, Valid: 75.86% Test: 73.61%\n",
            "Epoch: 243, Loss: 0.4280, Train: 77.47%, Valid: 75.40% Test: 73.49%\n",
            "Epoch: 244, Loss: 0.4223, Train: 77.62%, Valid: 75.22% Test: 73.14%\n",
            "Epoch: 245, Loss: 0.4209, Train: 78.02%, Valid: 75.45% Test: 73.57%\n",
            "Epoch: 246, Loss: 0.4265, Train: 78.20%, Valid: 75.90% Test: 73.58%\n",
            "Epoch: 247, Loss: 0.4252, Train: 78.59%, Valid: 75.92% Test: 73.58%\n",
            "Epoch: 248, Loss: 0.4212, Train: 78.69%, Valid: 75.77% Test: 73.81%\n",
            "Epoch: 249, Loss: 0.4223, Train: 79.10%, Valid: 76.17% Test: 73.64%\n",
            "Epoch: 250, Loss: 0.4210, Train: 78.80%, Valid: 75.89% Test: 73.83%\n",
            "Epoch: 251, Loss: 0.4206, Train: 79.16%, Valid: 75.92% Test: 73.97%\n",
            "Epoch: 252, Loss: 0.4197, Train: 78.45%, Valid: 75.80% Test: 73.91%\n",
            "Epoch: 253, Loss: 0.4182, Train: 78.65%, Valid: 75.98% Test: 73.82%\n",
            "Epoch: 254, Loss: 0.4207, Train: 78.10%, Valid: 75.49% Test: 73.02%\n",
            "Epoch: 255, Loss: 0.4233, Train: 78.87%, Valid: 75.91% Test: 73.23%\n",
            "Epoch: 256, Loss: 0.4222, Train: 77.85%, Valid: 75.79% Test: 73.46%\n",
            "Epoch: 257, Loss: 0.4168, Train: 78.71%, Valid: 75.77% Test: 73.54%\n",
            "Epoch: 258, Loss: 0.4188, Train: 78.47%, Valid: 76.13% Test: 74.07%\n",
            "Epoch: 259, Loss: 0.4249, Train: 77.95%, Valid: 75.97% Test: 74.08%\n",
            "Epoch: 260, Loss: 0.4233, Train: 78.34%, Valid: 76.33% Test: 74.12%\n",
            "Epoch: 261, Loss: 0.4227, Train: 77.89%, Valid: 76.30% Test: 73.89%\n",
            "Epoch: 262, Loss: 0.4190, Train: 79.00%, Valid: 75.86% Test: 73.82%\n",
            "Epoch: 263, Loss: 0.4183, Train: 78.91%, Valid: 76.13% Test: 73.96%\n",
            "Epoch: 264, Loss: 0.4154, Train: 78.71%, Valid: 75.79% Test: 73.77%\n",
            "Epoch: 265, Loss: 0.4146, Train: 78.60%, Valid: 75.78% Test: 73.97%\n",
            "Epoch: 266, Loss: 0.4215, Train: 78.31%, Valid: 76.07% Test: 74.46%\n",
            "Epoch: 267, Loss: 0.4188, Train: 78.47%, Valid: 76.45% Test: 74.00%\n",
            "Epoch: 268, Loss: 0.4217, Train: 78.31%, Valid: 76.19% Test: 74.25%\n",
            "Epoch: 269, Loss: 0.4197, Train: 78.54%, Valid: 76.18% Test: 74.68%\n",
            "Epoch: 270, Loss: 0.4164, Train: 78.82%, Valid: 76.57% Test: 74.85%\n",
            "Epoch: 271, Loss: 0.4174, Train: 78.38%, Valid: 76.52% Test: 75.19%\n",
            "Epoch: 272, Loss: 0.4175, Train: 78.55%, Valid: 76.69% Test: 74.54%\n",
            "Epoch: 273, Loss: 0.4184, Train: 78.67%, Valid: 76.23% Test: 74.47%\n",
            "Epoch: 274, Loss: 0.4151, Train: 78.01%, Valid: 75.97% Test: 73.22%\n",
            "Epoch: 275, Loss: 0.4119, Train: 78.50%, Valid: 75.37% Test: 72.65%\n",
            "Epoch: 276, Loss: 0.4181, Train: 78.39%, Valid: 75.48% Test: 73.07%\n",
            "Epoch: 277, Loss: 0.4174, Train: 78.00%, Valid: 75.41% Test: 73.51%\n",
            "Epoch: 278, Loss: 0.4162, Train: 78.25%, Valid: 75.37% Test: 73.81%\n",
            "Epoch: 279, Loss: 0.4136, Train: 78.57%, Valid: 75.87% Test: 74.16%\n",
            "Epoch: 280, Loss: 0.4178, Train: 78.60%, Valid: 76.46% Test: 74.38%\n",
            "Epoch: 281, Loss: 0.4167, Train: 78.37%, Valid: 75.80% Test: 74.00%\n",
            "Epoch: 282, Loss: 0.4095, Train: 79.14%, Valid: 76.33% Test: 74.01%\n",
            "Epoch: 283, Loss: 0.4097, Train: 79.16%, Valid: 76.00% Test: 74.65%\n",
            "Epoch: 284, Loss: 0.4160, Train: 78.78%, Valid: 76.28% Test: 74.30%\n",
            "Epoch: 285, Loss: 0.4079, Train: 78.92%, Valid: 76.08% Test: 74.09%\n",
            "Epoch: 286, Loss: 0.4177, Train: 78.59%, Valid: 75.59% Test: 73.93%\n",
            "Epoch: 287, Loss: 0.4116, Train: 78.63%, Valid: 75.71% Test: 73.18%\n",
            "Epoch: 288, Loss: 0.4100, Train: 78.85%, Valid: 75.92% Test: 73.27%\n",
            "Epoch: 289, Loss: 0.4161, Train: 79.16%, Valid: 75.87% Test: 73.12%\n",
            "Epoch: 290, Loss: 0.4154, Train: 78.68%, Valid: 75.25% Test: 73.03%\n",
            "Epoch: 291, Loss: 0.4099, Train: 78.98%, Valid: 75.61% Test: 72.36%\n",
            "Epoch: 292, Loss: 0.4111, Train: 78.80%, Valid: 75.02% Test: 71.90%\n",
            "Epoch: 293, Loss: 0.4126, Train: 78.50%, Valid: 74.75% Test: 71.99%\n",
            "Epoch: 294, Loss: 0.4177, Train: 78.93%, Valid: 74.52% Test: 71.49%\n",
            "Epoch: 295, Loss: 0.4109, Train: 78.36%, Valid: 73.26% Test: 70.89%\n",
            "Epoch: 296, Loss: 0.4188, Train: 78.53%, Valid: 74.07% Test: 71.12%\n",
            "Epoch: 297, Loss: 0.4147, Train: 78.07%, Valid: 74.82% Test: 71.77%\n",
            "Epoch: 298, Loss: 0.4102, Train: 79.06%, Valid: 75.29% Test: 73.00%\n",
            "Epoch: 299, Loss: 0.4104, Train: 78.40%, Valid: 75.56% Test: 73.82%\n",
            "Epoch: 300, Loss: 0.4150, Train: 78.49%, Valid: 76.05% Test: 73.74%\n",
            "Epoch: 301, Loss: 0.4084, Train: 78.65%, Valid: 75.66% Test: 73.64%\n",
            "Epoch: 302, Loss: 0.4112, Train: 78.56%, Valid: 75.57% Test: 73.64%\n",
            "Epoch: 303, Loss: 0.4041, Train: 78.49%, Valid: 75.70% Test: 73.60%\n",
            "Epoch: 304, Loss: 0.4082, Train: 78.36%, Valid: 75.99% Test: 74.04%\n",
            "Epoch: 305, Loss: 0.4127, Train: 77.88%, Valid: 75.38% Test: 74.22%\n",
            "Epoch: 306, Loss: 0.4108, Train: 77.85%, Valid: 75.33% Test: 73.87%\n",
            "Epoch: 307, Loss: 0.4102, Train: 77.90%, Valid: 74.89% Test: 73.72%\n",
            "Epoch: 308, Loss: 0.4099, Train: 77.78%, Valid: 75.00% Test: 73.64%\n",
            "Epoch: 309, Loss: 0.4110, Train: 77.10%, Valid: 75.39% Test: 73.81%\n",
            "Epoch: 310, Loss: 0.4064, Train: 78.05%, Valid: 74.88% Test: 73.85%\n",
            "Epoch: 311, Loss: 0.4132, Train: 76.82%, Valid: 75.11% Test: 73.35%\n",
            "Epoch: 312, Loss: 0.4092, Train: 77.72%, Valid: 74.33% Test: 73.53%\n",
            "Epoch: 313, Loss: 0.4123, Train: 76.69%, Valid: 74.60% Test: 73.03%\n",
            "Epoch: 314, Loss: 0.4110, Train: 77.24%, Valid: 74.99% Test: 72.73%\n",
            "Epoch: 315, Loss: 0.4083, Train: 78.03%, Valid: 74.53% Test: 72.64%\n",
            "Epoch: 316, Loss: 0.4085, Train: 77.65%, Valid: 74.78% Test: 72.82%\n",
            "Epoch: 317, Loss: 0.4080, Train: 78.03%, Valid: 75.17% Test: 72.72%\n",
            "Epoch: 318, Loss: 0.4031, Train: 78.56%, Valid: 75.17% Test: 73.31%\n",
            "Epoch: 319, Loss: 0.4077, Train: 78.66%, Valid: 75.71% Test: 73.17%\n",
            "Epoch: 320, Loss: 0.4086, Train: 77.83%, Valid: 75.36% Test: 73.13%\n",
            "Epoch: 321, Loss: 0.4102, Train: 77.33%, Valid: 74.77% Test: 73.05%\n",
            "Epoch: 322, Loss: 0.4077, Train: 77.81%, Valid: 75.52% Test: 73.14%\n",
            "Epoch: 323, Loss: 0.4073, Train: 77.69%, Valid: 75.37% Test: 73.74%\n",
            "Epoch: 324, Loss: 0.4051, Train: 78.53%, Valid: 75.27% Test: 73.53%\n",
            "Epoch: 325, Loss: 0.4058, Train: 78.68%, Valid: 75.77% Test: 73.75%\n",
            "Epoch: 326, Loss: 0.4083, Train: 77.99%, Valid: 75.84% Test: 74.08%\n",
            "Epoch: 327, Loss: 0.4063, Train: 78.25%, Valid: 76.43% Test: 73.98%\n",
            "Epoch: 328, Loss: 0.4072, Train: 79.20%, Valid: 76.38% Test: 74.06%\n",
            "Epoch: 329, Loss: 0.4068, Train: 79.35%, Valid: 75.79% Test: 73.75%\n",
            "Epoch: 330, Loss: 0.4096, Train: 79.11%, Valid: 75.50% Test: 73.43%\n",
            "Epoch: 331, Loss: 0.4078, Train: 79.48%, Valid: 75.35% Test: 72.85%\n",
            "Epoch: 332, Loss: 0.4055, Train: 79.11%, Valid: 75.50% Test: 72.69%\n",
            "Epoch: 333, Loss: 0.4041, Train: 78.80%, Valid: 75.93% Test: 72.96%\n",
            "Epoch: 334, Loss: 0.4051, Train: 79.19%, Valid: 74.84% Test: 71.89%\n",
            "Epoch: 335, Loss: 0.4097, Train: 78.38%, Valid: 74.81% Test: 71.87%\n",
            "Epoch: 336, Loss: 0.4051, Train: 78.76%, Valid: 75.22% Test: 72.53%\n",
            "Epoch: 337, Loss: 0.4045, Train: 78.95%, Valid: 75.23% Test: 73.01%\n",
            "Epoch: 338, Loss: 0.4064, Train: 78.22%, Valid: 75.70% Test: 73.26%\n",
            "Epoch: 339, Loss: 0.4036, Train: 79.36%, Valid: 75.60% Test: 73.63%\n",
            "Epoch: 340, Loss: 0.4095, Train: 78.47%, Valid: 74.98% Test: 72.97%\n",
            "Epoch: 341, Loss: 0.4082, Train: 78.30%, Valid: 75.06% Test: 72.09%\n",
            "Epoch: 342, Loss: 0.4003, Train: 79.40%, Valid: 75.48% Test: 72.32%\n",
            "Epoch: 343, Loss: 0.4076, Train: 78.70%, Valid: 75.19% Test: 73.11%\n",
            "Epoch: 344, Loss: 0.4071, Train: 79.28%, Valid: 76.11% Test: 73.72%\n",
            "Epoch: 345, Loss: 0.3989, Train: 79.10%, Valid: 75.67% Test: 73.64%\n",
            "Epoch: 346, Loss: 0.4057, Train: 78.80%, Valid: 76.18% Test: 73.58%\n",
            "Epoch: 347, Loss: 0.4078, Train: 79.34%, Valid: 76.31% Test: 73.67%\n",
            "Epoch: 348, Loss: 0.4025, Train: 79.10%, Valid: 75.86% Test: 73.81%\n",
            "Epoch: 349, Loss: 0.4035, Train: 78.97%, Valid: 76.41% Test: 74.25%\n",
            "Epoch: 350, Loss: 0.4040, Train: 78.77%, Valid: 76.43% Test: 74.08%\n",
            "Epoch: 351, Loss: 0.4074, Train: 78.55%, Valid: 75.76% Test: 74.07%\n",
            "Epoch: 352, Loss: 0.4039, Train: 79.70%, Valid: 76.32% Test: 73.71%\n",
            "Epoch: 353, Loss: 0.4059, Train: 78.94%, Valid: 76.02% Test: 74.09%\n",
            "Epoch: 354, Loss: 0.3962, Train: 78.50%, Valid: 76.09% Test: 73.81%\n",
            "Epoch: 355, Loss: 0.4031, Train: 78.78%, Valid: 75.71% Test: 73.01%\n",
            "Epoch: 356, Loss: 0.4052, Train: 78.50%, Valid: 74.73% Test: 72.44%\n",
            "Epoch: 357, Loss: 0.4030, Train: 77.96%, Valid: 74.30% Test: 72.10%\n",
            "Epoch: 358, Loss: 0.4046, Train: 77.62%, Valid: 73.51% Test: 70.86%\n",
            "Epoch: 359, Loss: 0.4014, Train: 77.94%, Valid: 73.52% Test: 70.73%\n",
            "Epoch: 360, Loss: 0.4051, Train: 77.30%, Valid: 73.12% Test: 70.53%\n",
            "Epoch: 361, Loss: 0.4024, Train: 76.72%, Valid: 73.12% Test: 70.15%\n",
            "Epoch: 362, Loss: 0.4064, Train: 77.21%, Valid: 72.93% Test: 70.38%\n",
            "Epoch: 363, Loss: 0.4022, Train: 77.21%, Valid: 73.58% Test: 70.83%\n",
            "Epoch: 364, Loss: 0.4038, Train: 78.40%, Valid: 73.82% Test: 71.09%\n",
            "Epoch: 365, Loss: 0.3994, Train: 77.96%, Valid: 74.46% Test: 71.60%\n",
            "Epoch: 366, Loss: 0.4031, Train: 78.53%, Valid: 74.77% Test: 71.79%\n",
            "Epoch: 367, Loss: 0.4011, Train: 79.28%, Valid: 74.98% Test: 71.83%\n",
            "Epoch: 368, Loss: 0.4052, Train: 79.22%, Valid: 75.26% Test: 72.31%\n",
            "Epoch: 369, Loss: 0.4010, Train: 79.28%, Valid: 75.52% Test: 72.64%\n",
            "Epoch: 370, Loss: 0.4039, Train: 78.69%, Valid: 75.80% Test: 72.97%\n",
            "Epoch: 371, Loss: 0.4030, Train: 78.44%, Valid: 75.81% Test: 72.53%\n",
            "Epoch: 372, Loss: 0.3926, Train: 78.87%, Valid: 75.29% Test: 72.78%\n",
            "Epoch: 373, Loss: 0.4043, Train: 78.35%, Valid: 75.85% Test: 72.99%\n",
            "Epoch: 374, Loss: 0.4006, Train: 78.87%, Valid: 75.56% Test: 72.90%\n",
            "Epoch: 375, Loss: 0.3947, Train: 79.23%, Valid: 75.52% Test: 73.08%\n",
            "Epoch: 376, Loss: 0.3959, Train: 78.22%, Valid: 75.90% Test: 73.60%\n",
            "Epoch: 377, Loss: 0.4027, Train: 77.89%, Valid: 75.62% Test: 73.95%\n",
            "Epoch: 378, Loss: 0.4021, Train: 78.58%, Valid: 76.43% Test: 74.19%\n",
            "Epoch: 379, Loss: 0.4030, Train: 79.07%, Valid: 76.82% Test: 74.45%\n",
            "Epoch: 380, Loss: 0.3996, Train: 79.46%, Valid: 76.69% Test: 73.56%\n",
            "Epoch: 381, Loss: 0.4074, Train: 78.79%, Valid: 76.35% Test: 73.32%\n",
            "Epoch: 382, Loss: 0.3956, Train: 79.53%, Valid: 75.73% Test: 73.28%\n",
            "Epoch: 383, Loss: 0.4042, Train: 79.04%, Valid: 76.60% Test: 73.57%\n",
            "Epoch: 384, Loss: 0.3978, Train: 78.68%, Valid: 75.97% Test: 73.32%\n",
            "Epoch: 385, Loss: 0.4029, Train: 78.36%, Valid: 76.04% Test: 73.34%\n",
            "Epoch: 386, Loss: 0.4037, Train: 78.66%, Valid: 76.01% Test: 73.44%\n",
            "Epoch: 387, Loss: 0.3998, Train: 78.56%, Valid: 75.85% Test: 73.92%\n",
            "Epoch: 388, Loss: 0.3981, Train: 78.21%, Valid: 76.07% Test: 74.25%\n",
            "Epoch: 389, Loss: 0.4003, Train: 78.80%, Valid: 76.55% Test: 74.93%\n",
            "Epoch: 390, Loss: 0.4003, Train: 78.51%, Valid: 76.84% Test: 75.15%\n",
            "Epoch: 391, Loss: 0.4019, Train: 78.59%, Valid: 77.40% Test: 74.76%\n",
            "Epoch: 392, Loss: 0.3953, Train: 79.38%, Valid: 76.73% Test: 74.54%\n",
            "Epoch: 393, Loss: 0.3943, Train: 79.27%, Valid: 76.77% Test: 75.01%\n",
            "Epoch: 394, Loss: 0.3989, Train: 78.56%, Valid: 76.49% Test: 74.96%\n",
            "Epoch: 395, Loss: 0.3936, Train: 79.31%, Valid: 76.73% Test: 75.21%\n",
            "Epoch: 396, Loss: 0.3984, Train: 78.38%, Valid: 76.78% Test: 75.22%\n",
            "Epoch: 397, Loss: 0.3997, Train: 78.49%, Valid: 75.98% Test: 74.78%\n",
            "Epoch: 398, Loss: 0.3999, Train: 79.31%, Valid: 75.99% Test: 74.21%\n",
            "Epoch: 399, Loss: 0.3897, Train: 79.63%, Valid: 75.53% Test: 73.40%\n",
            "Epoch: 400, Loss: 0.3952, Train: 79.30%, Valid: 76.03% Test: 73.42%\n",
            "Epoch: 401, Loss: 0.3983, Train: 79.31%, Valid: 76.68% Test: 74.07%\n",
            "Epoch: 402, Loss: 0.3919, Train: 79.11%, Valid: 76.48% Test: 74.52%\n",
            "Epoch: 403, Loss: 0.3950, Train: 78.47%, Valid: 76.54% Test: 74.32%\n",
            "Epoch: 404, Loss: 0.3958, Train: 78.32%, Valid: 76.53% Test: 74.39%\n",
            "Epoch: 405, Loss: 0.3940, Train: 78.57%, Valid: 76.17% Test: 73.96%\n",
            "Epoch: 406, Loss: 0.3997, Train: 78.56%, Valid: 76.07% Test: 74.83%\n",
            "Epoch: 407, Loss: 0.3937, Train: 78.22%, Valid: 76.05% Test: 74.97%\n",
            "Epoch: 408, Loss: 0.3957, Train: 77.94%, Valid: 76.60% Test: 74.58%\n",
            "Epoch: 409, Loss: 0.3989, Train: 78.53%, Valid: 76.28% Test: 74.58%\n",
            "Epoch: 410, Loss: 0.3966, Train: 78.61%, Valid: 76.68% Test: 74.33%\n",
            "Epoch: 411, Loss: 0.4017, Train: 78.97%, Valid: 75.92% Test: 73.75%\n",
            "Epoch: 412, Loss: 0.3957, Train: 79.30%, Valid: 76.47% Test: 73.71%\n",
            "Epoch: 413, Loss: 0.3965, Train: 78.86%, Valid: 75.90% Test: 73.85%\n",
            "Epoch: 414, Loss: 0.3919, Train: 78.08%, Valid: 76.05% Test: 73.42%\n",
            "Epoch: 415, Loss: 0.3958, Train: 78.56%, Valid: 76.04% Test: 73.65%\n",
            "Epoch: 416, Loss: 0.3995, Train: 79.11%, Valid: 76.32% Test: 73.60%\n",
            "Epoch: 417, Loss: 0.3936, Train: 79.27%, Valid: 76.40% Test: 73.03%\n",
            "Epoch: 418, Loss: 0.3965, Train: 79.01%, Valid: 75.86% Test: 73.42%\n",
            "Epoch: 419, Loss: 0.3915, Train: 78.47%, Valid: 76.15% Test: 73.94%\n",
            "Epoch: 420, Loss: 0.3940, Train: 78.18%, Valid: 76.43% Test: 73.66%\n",
            "Epoch: 421, Loss: 0.3960, Train: 78.24%, Valid: 75.80% Test: 73.74%\n",
            "Epoch: 422, Loss: 0.3957, Train: 79.20%, Valid: 75.86% Test: 73.21%\n",
            "Epoch: 423, Loss: 0.3868, Train: 79.08%, Valid: 75.64% Test: 72.76%\n",
            "Epoch: 424, Loss: 0.3933, Train: 79.10%, Valid: 75.93% Test: 72.95%\n",
            "Epoch: 425, Loss: 0.3968, Train: 79.24%, Valid: 76.26% Test: 72.96%\n",
            "Epoch: 426, Loss: 0.3957, Train: 79.26%, Valid: 76.08% Test: 72.96%\n",
            "Epoch: 427, Loss: 0.3894, Train: 79.30%, Valid: 75.32% Test: 72.49%\n",
            "Epoch: 428, Loss: 0.3939, Train: 79.49%, Valid: 74.96% Test: 72.06%\n",
            "Epoch: 429, Loss: 0.3932, Train: 79.52%, Valid: 75.59% Test: 72.56%\n",
            "Epoch: 430, Loss: 0.3958, Train: 78.96%, Valid: 75.79% Test: 73.05%\n",
            "Epoch: 431, Loss: 0.3975, Train: 78.32%, Valid: 75.66% Test: 72.60%\n",
            "Epoch: 432, Loss: 0.3915, Train: 78.32%, Valid: 74.80% Test: 71.99%\n",
            "Epoch: 433, Loss: 0.3907, Train: 77.94%, Valid: 74.37% Test: 71.51%\n",
            "Epoch: 434, Loss: 0.3899, Train: 77.70%, Valid: 74.52% Test: 72.25%\n",
            "Epoch: 435, Loss: 0.3937, Train: 78.47%, Valid: 74.83% Test: 72.72%\n",
            "Epoch: 436, Loss: 0.3969, Train: 78.13%, Valid: 75.04% Test: 73.14%\n",
            "Epoch: 437, Loss: 0.3901, Train: 78.82%, Valid: 75.59% Test: 73.28%\n",
            "Epoch: 438, Loss: 0.3903, Train: 78.74%, Valid: 75.36% Test: 72.92%\n",
            "Epoch: 439, Loss: 0.3923, Train: 78.92%, Valid: 74.79% Test: 72.24%\n",
            "Epoch: 440, Loss: 0.3960, Train: 78.61%, Valid: 73.72% Test: 71.39%\n",
            "Epoch: 441, Loss: 0.3929, Train: 78.68%, Valid: 73.79% Test: 70.83%\n",
            "Epoch: 442, Loss: 0.3893, Train: 78.53%, Valid: 74.39% Test: 71.26%\n",
            "Epoch: 443, Loss: 0.3963, Train: 79.49%, Valid: 74.87% Test: 72.13%\n",
            "Epoch: 444, Loss: 0.3940, Train: 78.86%, Valid: 75.15% Test: 72.03%\n",
            "Epoch: 445, Loss: 0.3861, Train: 79.85%, Valid: 75.75% Test: 72.92%\n",
            "Epoch: 446, Loss: 0.3954, Train: 78.82%, Valid: 75.19% Test: 73.19%\n",
            "Epoch: 447, Loss: 0.3896, Train: 77.72%, Valid: 75.72% Test: 73.33%\n",
            "Epoch: 448, Loss: 0.3937, Train: 76.24%, Valid: 74.69% Test: 72.93%\n",
            "Epoch: 449, Loss: 0.3957, Train: 75.50%, Valid: 74.39% Test: 73.31%\n",
            "Epoch: 450, Loss: 0.3950, Train: 76.59%, Valid: 74.77% Test: 73.54%\n",
            "Epoch: 451, Loss: 0.3980, Train: 76.52%, Valid: 74.95% Test: 74.04%\n",
            "Epoch: 452, Loss: 0.3892, Train: 77.37%, Valid: 75.53% Test: 74.35%\n",
            "Epoch: 453, Loss: 0.3915, Train: 78.38%, Valid: 75.85% Test: 74.66%\n",
            "Epoch: 454, Loss: 0.3894, Train: 78.50%, Valid: 76.95% Test: 74.75%\n",
            "Epoch: 455, Loss: 0.3885, Train: 79.74%, Valid: 76.42% Test: 74.02%\n",
            "Epoch: 456, Loss: 0.3962, Train: 79.11%, Valid: 75.50% Test: 72.84%\n",
            "Epoch: 457, Loss: 0.3958, Train: 78.91%, Valid: 74.27% Test: 71.25%\n",
            "Epoch: 458, Loss: 0.3896, Train: 79.00%, Valid: 73.89% Test: 71.37%\n",
            "Epoch: 459, Loss: 0.3878, Train: 79.07%, Valid: 75.00% Test: 72.35%\n",
            "Epoch: 460, Loss: 0.3872, Train: 78.67%, Valid: 74.77% Test: 72.60%\n",
            "Epoch: 461, Loss: 0.3904, Train: 78.24%, Valid: 74.82% Test: 72.50%\n",
            "Epoch: 462, Loss: 0.3884, Train: 78.28%, Valid: 74.37% Test: 72.53%\n",
            "Epoch: 463, Loss: 0.3879, Train: 77.91%, Valid: 75.02% Test: 71.96%\n",
            "Epoch: 464, Loss: 0.3875, Train: 76.58%, Valid: 74.07% Test: 72.28%\n",
            "Epoch: 465, Loss: 0.3852, Train: 76.14%, Valid: 73.05% Test: 72.12%\n",
            "Epoch: 466, Loss: 0.3954, Train: 75.62%, Valid: 73.69% Test: 72.36%\n",
            "Epoch: 467, Loss: 0.3886, Train: 77.03%, Valid: 74.34% Test: 72.93%\n",
            "Epoch: 468, Loss: 0.3882, Train: 77.60%, Valid: 74.58% Test: 73.24%\n",
            "Epoch: 469, Loss: 0.3863, Train: 78.05%, Valid: 74.64% Test: 72.81%\n",
            "Epoch: 470, Loss: 0.3934, Train: 77.63%, Valid: 74.61% Test: 72.65%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-86090fe8c0cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                               torch.zeros(valid_negative_samples.shape[0])))\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalid_eval_outfits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_negative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     valid_acc = test(model, x, valid_message_passing_edges.to(device)\n\u001b[0m\u001b[1;32m     27\u001b[0m     , valid_eval_outfits.to(device), valid_labels.to(device))\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4535d83d6ded>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, x, edge_index, outfits, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0membds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mo_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mo_pred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mo_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## results:\n",
        "We achived the best test result of 75.22 in epoch 396. In addition, we think that we will achieve better results, if we use a stronger approach for negative sampling, like using compatiblity dataset that is available.\n",
        "\n",
        "in the next section, we will train our model on a dataset with better negative samples, but also the same positive ones."
      ],
      "metadata": {
        "id": "2qvm4HEY7FTd"
      }
    }
  ]
}